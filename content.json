{"pages":[{"title":"404","text":"找不到页面了！","path":"404/index.html","date":"01-18","excerpt":""},{"title":"这里是爱吃咸鱼的喵 _(:з」」","text":".topimg img{ box-shadow: 10px 10px 10px rgba(0,0,0,.5); border-radius:20px; /*考虑浏览器兼容性*/ -moz-box-shadow: 10px 10px 10px rgba(0,0,0,.5); -webkit-box-shadow: 10px 10px 10px rgba(0,0,0,.5); border: 5px solid #ddd; padding: 5px; background: #fff; } a{ color: blue } #dd #left{ width:350px; /* background-color:yellow; */ float:left; padding-bottom:3px; } #dd #right{ /* background-color:aqua; */ margin-left:390px; padding-right:20px; padding-bottom:3px; } #dd img{ box-shadow: 10px 10px 10px rgba(0,0,0,.5); border-radius:20px; /*考虑浏览器兼容性*/ -moz-box-shadow: 10px 10px 10px rgba(0,0,0,.5); -webkit-box-shadow: 10px 10px 10px rgba(0,0,0,.5); margin-bottom: 7px; border: 2px solid #ddd; padding: 2px; background: #fff; } 个人介绍 死宅 半死宅一名，喜欢看动漫，最爱寒蝉鸣泣之时和命运石之门 转行程序员中，喜欢折腾各种东西，目前最感兴趣的东西： 深度学习CV 物体检测 目标跟踪 GAN 机器学习 ACM，算法与数据结构 嵌入式编程 目前自己还是一直小菜鸡，也是希望能够在博客里面坚持记录自己的学习经历，多总结才有收货 联系方式 喜欢听音乐、看动漫 以前会玩游戏现在很少玩了 小菜鸡很多东西都在学习中... 努力成为一名算法工程师调参侠 需要友情链接的直接留言或者Github提交ISSUE &emsp;e-mailzqhzju@zju.edu.cn &emsp;GithubJJXiangJiaoJun &emsp;CSDNXiangJiaoJun_的博客 &emsp;Twitter爱吃咸鱼的喵 &emsp;Weibo黯夜泡沫 后记 一直在成长中，每天进步一点点～ლ(•̀ _ •́ ლ) 欢迎e-mail我和我讨论技术问题~","path":"about/index.html","date":"01-18","excerpt":""},{"title":"分类","text":"123","path":"categories/index.html","date":"01-18","excerpt":""},{"title":"留言板","text":".topimg img{ box-shadow: 10px 10px 10px rgba(0,0,0,.5); border-radius:20px; /*考虑浏览器兼容性*/ -moz-box-shadow: 10px 10px 10px rgba(0,0,0,.5); -webkit-box-shadow: 10px 10px 10px rgba(0,0,0,.5); border: 5px solid #ddd; padding: 5px; background: #fff; } 说明欢迎来到我的个人博客(๑•̀ㅂ•́)و✧，博客刚刚才建起来，还有许多没有完善的地方，有什么问题在下面留言就行了，窝抽空一定会看的,直接用github账号登录就能评论了。 也不知道什么时候有空23333 123+ 持续更新中．．．．．．+ 2019年1月19日正式发布- 2019年１月16日开始搭建 完成度 [x] 个人主页 [x] 评论系统 [x] 文章统计 [x] 看板娘 [ ] 时间轴 [ ] 一些额外的功能 如果博客添加了一些额外的功能我也会在这里发布~ 更新内容博客的主题主要包含： 深度学习 (主要是PyTorch和MXNet的学习) 算法及数据结构 论文笔记 Java后端 (说实话我已经放弃学了) python的一些学习笔记 科学上网的一些教程 (TODO) Ubuntu系统的一些配置教程 (TODO) 我写博客也是个新手，感觉排版什么的都不忍直视,markdown和html也在努力学习中，欢迎大家在评论区diss我：）,我会好好改正的～ 联系我请移步关于我,欢迎大家e-mail和我讨论技术问题。 友情链接需要添加友情链接，直接留言或者e-mail告诉我就可以了_(:3」∠)_ 最后，希望大家在我的小站玩的愉快,也希望我的博客能给你带来一点帮助哔哩哔哩 (゜-゜)つロ 干杯~","path":"message/index.html","date":"01-19","excerpt":""},{"title":"","text":"img{ box-shadow: 10px 10px 10px rgba(0,0,0,.5); /*考虑浏览器兼容性*/ -moz-box-shadow: 10px 10px 10px rgba(0,0,0,.5); -webkit-box-shadow: 10px 10px 10px rgba(0,0,0,.5); }","path":"message/message.css","date":"01-23","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"01-18","excerpt":""},{"title":"标签","text":"","path":"tags/index.html","date":"01-18","excerpt":""}],"posts":[{"title":"安装Ubuntu系统必做的事情（常用软件安装＋系统配置）","text":"&emsp;&emsp;如果你是一个程序员，那你会爱上Linux系统～相比Windows，在Linux系统下，我们更能享受强大的命令行带给我们的高效，以及一些软件环境配置的方便、简介。然而原装Ubuntu系统并不是那么的好用，我们进行一些必要软件的安装和配置之后，才能让Ubuntu性能MAX~ &emsp;&emsp;作为一个经常把操作系统弄坏的人，感觉每次重装玩系统之后配置都是一件繁琐的工作，每次都是配置那么些个东西，命令也就是那么几个，没错说的就是(sudo apt-get、git clone)。网上很多博客都有一些常用软件的安装教程，不过找的时候比较混乱，这里总结一下Ubuntu安装后，一些必要软件的安装和系统的配置，希望教程能方便重装系统后需要配置的小伙伴，跟着这篇教程一步步配置，你就能打造一个专属你的强大的Ubuntu系统啦 参考资料: 安装Ubuntu后应该做的事 ubuntu的一些基本配置 Ubuntu系统的配置美化 注意: 12+本教程在 Ubuntu18.04LTS 进行安装测试-电脑型号为联想拯救者R720 首先，窝晒一波最后配置好的Ubuntu系统~ 修改国内镜像源 设置root密码 安装必备的软件和工具 安装搜狗输入法 安装Chrome Ubuntu桌面美化（unity-tweak-tool） 安装WPS 安装截图工具shutter 安装网易云音乐 安装vscode 安装系统指示器syspeek 科学上网安装ssr Privoxy配置(实现终端科学上网) 安装Anaconda 安装PyCharm 安装NVIDIA驱动 卸载firefox浏览器 卸载一些用不到的自带软件 删除libreoffice，Amazon的链接 解决Win10、Ubuntu双系统时间同步问题 结语 修改国内镜像源&emsp;&emsp;不解释，这个必须第一步就做，Ubuntu系统官方的源服务器在国外，下载速度对于我们国内用户简直就是噩梦，而我们之后安装软件，更新软件都是从镜像源下载，如果不更换源，就会获得龟速下载debuff，谁也不想用2kb/s的下载速度去下载配置软件吧。我们第一步就需要把源更换为国内镜像，享受国内的高速下载，配置很简单如下： 首先备份source.list文件 1sudo cp /etc/apt/sources.list /etc/apt/sources.list.bkp 用gedit打开sources.list 1sudo gedit /etc/apt/sources.list 将source.list内容全部替换为清华源 12345678910deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse 最后运行命令更新系统源 12sudo apt-get updatesudo apt-get upgrade 设置root密码&emsp;&emsp;Ubuntu安装后默认没有root密码，需要用户自己设置。 123456sudo su -##输入账户密码passwd##输入新的root密码##再次输入root密码 安装必备的软件和工具首先运行下面代码 1sudo apt-get install build-essential Git&emsp;&emsp;Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。 1sudo apt-get install git 配置用户名和邮箱 12git config --global user.name \"yourname\"git config --global user.email \"your e-mail address\" Vim&emsp;&emsp;vim是Linux系统下一款常用的文本编辑工具，可以安装各种插件从而配置为强大的IDE,高手都直接用Vim编写代码，必装 1sudo apt-get install vim wget&emsp;&emsp;wget是Linux系统下一款强大的命令行下载工具，支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议 下载，并可以使用 HTTP 代理,超好用的下载工具，很方便，必装 123sudo apt-get updatesudo apt-get install wgetwget --version ＃查看版本是否安装成功 curl&emsp;&emsp;cURL是一个利用URL语法在命令行下工作的文件传输工具，1997年首次发行。它支持文件上传和下载，所以是综合传输工具，但按传统，习惯称cURL为下载工具,和wget差不多,也是一个命令行下载工具 1sudo apt install curl cmake&emsp;&emsp;CMake是一个跨平台的安装(编译)工具,可以用简单的语句来描述所有平台的安装(编译过程)。他能够输出各种各样的makefile或者project文件,能测试编译器所支持的C++特性,类似UNIX下的automake。这里直接用apt安装，也可以从源码安装1sudo apt-get install cmake g++&emsp;&emsp;C++编译器 1sudo apt-get install g++ 安装搜狗输入法&emsp;&emsp;下一步我们要安装一个好用的中文输入法，没错就是搜狗输入法，Ubuntu默认的输入法框架ibus不怎么好用，步骤如下所示: 安装fcitx输入法框架 1234567sudo apt-get install fcitx-bin #安装fcitx-binsudo apt-get update --fix-missing #修复fcitx-bin安装失败的情况sudo apt-get install fcitx-bin #重新安装fcitx-binsudo apt-get install fcitx-table #安装fcitx-table 安装搜狗输入法linux版本 首先去官网下载搜狗输入法Linux版本.deb包,链接如下搜狗输入法下载，然后进入到下载目录，运行如下命令: 12345sudo dpkg -i sogoupinyin*.deb #安装搜狗拼音sudo apt-get install -f #修复搜狗拼音安装的错误sudo dpkg -i sogoupinyin*.deb #重新安装搜狗拼音 配置搜狗输入法 安装完成之后还不能直接使用输入法，我们需要在系统中进行配置，具体参考这篇博客Ubuntu18.04下安装搜狗输入法 安装Chrome&emsp;&emsp;我个人比较喜欢Chrome浏览器，如果你习惯用firefox浏览器那就不用安装啦。我们首先到Chrome官网下载最新的.deb安装包，然后运行如下命令，插件安装什么的就不在这里说啦。 123sudo apt-get install libappindicator1 libindicator7sudo dpkg -i Downloads/google-chrome-stable_current_amd64.debsudo apt-get -f install Ubuntu桌面美化（unity-tweak-tool）&emsp;&emsp;Ubuntu原生的主题和终端很丑有木有，一点都不想在这么丑的界面上工作。安装gnome-tweak-tool能真正的打造属于我们自己的Ubuntu系统，实现各种系统美化和主题替换。 首先安装gnome软件 1sudo apt install gnome-tweak-tool 配置gname-tweaks &emsp;&emsp;安装好之后我们可以在终端输入 gnome-tweaks 或者直接在软件中查找tweaks打开。配置如下，我们可以在Themes中配置主题，还有Shell，另外还有许多其他的选项，这里就不详细说了。更多细节请参考这篇博客利用gnome美化Ubuntu18.04,但此时shell主题的切换默认是加锁的，我们还需要安装扩展。安装后我们只需找到 Tweaks--&gt;Extensions--&gt;User themes然后把启动就可以随意替换我们自己的shell主题啦~ 安装chrome-gnome-shell 1sudo apt install chrome-gnome-shell &emsp;&emsp;安装这个是为了在浏览器里面能一键安装插件（方便）,进入这个网站就可以直接安装了https://extensions.gnome.org/ 1sudo apt install gnome-shell-extensions 这里直接安装8-10个扩展，然后我们需要重启一下系统就可以使用了。 替换主题 我们可以在https://www.gnome-look.org/这个网站上下载主题，然后进行替换，具体操作请参考博客Linux也可以这样美——Ubuntu18.04安装、配置、美化-踩坑记 安装WPS&emsp;&emsp;我们要在Ubuntu上看PPT和Word的话，用WPS最合适了。首先到WPS官网下载最新安装包，然后运行 12sudo dpkg -i ~/Downloads/wps-office*.debsudo apt-get -f install 安装截图工具shutter&emsp;&emsp;截图后不能编辑的解决办法请看这篇博客,Ubuntu 18.04中截图工具Shutter的编辑按钮不可用的解决办法 1sudo apt-get install shutter 安装网易云音乐&emsp;&emsp;首先到网易云音乐官网下载最新安装包，然后运行 12sudo dpkg -i ~/Downloads/netease-cloud-music*.debsudo apt-get -f install 安装vscode&emsp;&emsp;超级好用的代码编辑器，可以安装各种插件，功能非常强大。首先到vscode官网下载最新安装包，然后运行 12sudo dpkg -i ~/Downloads/code*.debsudo apt-get -f install 安装系统指示器syspeek&emsp;&emsp;可以监控系统运行的状态12sudo add-apt-repository ppa:nilarimogard/webupd8sudo apt-get update &amp;&amp; sudo apt-get install -y syspeek 科学上网安装ssr&emsp;&emsp;注意最好先把系统默认的python版本改为python3,运行ssr config之后，设置好你自己的服务器代理，最后在chrome上安装swithomega配置就好了，swithomega具体配置请看教程使用SwitchyOmega设置Chrome代理 12345wget http://www.djangoz.com/ssrsudo mv ssr /usr/local/binsudo chmod 766 /usr/local/bin/ssrssr installssr config Privoxy配置(实现终端科学上网) 安装Privoxy 1sudo apt install -y privoxy 配置Privoxy 12345678# 添加本地ssr服务到配置文件echo 'forward-socks5 / 127.0.0.1:1080 .' &gt;&gt; /etc/privoxy/config# Privoxy 默认监听端口是是8118export http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118export no_proxy=localhost# 启动服务systemctl start privoxy.service proxy 环境变量 &emsp;&emsp;把下面的代码添加到 ~/.bashrc的最后 12sudo vim ~/.bashrcsource ~/.bashrc 12345678# privoxy默认监听端口为8118export http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118export no_proxy=localhost# no_proxy是不经过privoxy代理的地址# 只能填写具体的ip、域名后缀，多个条目之间使用','逗号隔开# 比如: export no_proxy=\"localhost, 192.168.1.1, ip.cn, chinaz.com\"# 访问 localhost、192.168.1.1、ip.cn、*.ip.cn、chinaz.com、*.chinaz.com 将不使用代理 代理测试 &emsp;&emsp;访问各大网站，如果都有网页源码输出说明代理没问题 1234567891011121314curl -sL www.baidu.comcurl -sL www.google.comcurl -sL www.google.com.hkcurl -sL www.google.co.jpcurl -sL www.youtube.comcurl -sL mail.google.comcurl -sL facebook.comcurl -sL twitter.comcurl -sL www.wikipedia.org# 获取当前 IP 地址# 如果使用 privoxy 全局模式，则应该显示 ss 服务器的 IP# 如果使用 privoxy gfwlist模式，则应该显示本地公网 IPcurl -sL ip.chinaz.com/getip.aspx 管理脚本 123456789101112131415161718192021222324252627#!/bin/bash# Author Samzong.lucase $1 in start) ssr start &amp;&gt; /var/log/ssr-local.log systemctl start privoxy.service export http_proxy=http://127.0.0.1:8118 export https_proxy=http://127.0.0.1:8118 export no_proxy=\"localhost, ip.cn, chinaz.com\" ;; stop) unset http_proxy https_proxy no_proxy systemctl stop privoxy.service ssr stop &amp;&gt; /var/log/ssr-log.log ;; autostart) echo \"ssr start\" &gt;&gt; /etc/rc.local systemctl enable privoxy.service echo \"http_proxy=http://127.0.0.1:8118\" &gt;&gt; /etc/bashrc echo \"https_proxy=http://127.0.0.1:8118\" &gt;&gt; /etc/bashrc echo \"no_proxy='localhost, ip.cn, chinaz.com'\" &gt;&gt; /etc/bashrc ;; *) echo \"usage: source $0 start|stop|autostart\" exit 1 ;;esac 使用 12345678mv gfwlist2privoxy/ssr_manager /usr/local/binchmod +x ssr_manager# 启动服务ssr_manager start# 关闭服务ssr_manager stop # 添加开机自启动ssr_manager autostart 安装Anaconda&emsp;&emsp;Anaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。安装Anaconda之后，基本所有的科学计算包都已经有了，我们就不用一个个去pip install 了。 下载Anaconda 在官网下载对应的Anaconda runfile文件 安装Anaconda &emsp;&emsp;进入到下载目录安装Anaconda 1sudo sh ~/Downloads/Anaconda*.sh 检查安装是否成功 &emsp;&emsp;重启终端,然后输入python，检查默认的python是否已经修改为Anaconda中的python 安装PyCharm&emsp;&emsp;个人比较喜欢用PyCharm来写Python代码，喜欢的同学可以按照下面的步骤来装。 下载PyCharm 首先去官网下载Professional专业版 解压文件 1sudo tar -xzvf pycharm-*.tar.gz 安装Pycharm 进入解压目录，并运行pycharm.sh文件 12cd ~/Downloads/pycharm-2018.1.4/binsh ./pycharm.sh 激活Pycharm 剩下的激活步骤请看博客,Ubuntu 18.04 安装 PyCharm里面详细介绍了激活以及后续的一些配置。 安装NVIDIA驱动&emsp;&emsp;如果你的电脑有独立显卡，那么不安装NVIDIA驱动的话会出现各种重启卡死，关机卡死等等问题QAQ,所以nvidia显卡驱动是必须装的，一了百了解决各种问题。 卸载原有驱动 1sudo apt-get remove --purge nvidia* 添加nivdia驱动ppa源 123sudo add-apt-repository ppa:xorg-edgers/ppa #添加ppa源sudo add-apt-repository ppa:graphics-drivers/ppa #添加ppa源sudo apt-get update #更新apt-get 打开Software &amp; Updates 进行驱动安装 打开Software &amp; Updates ---&gt; Additional Drivers选择合适的nvidia驱动安装，如下图所示： 重启电脑，检查安装是否成功 12nvidia-smi #若列出GPU的信息列表，表示驱动安装成功nvidia-settings #若弹出设置对话框，亦表示驱动安装成功 卸载firefox浏览器&emsp;&emsp;如果你已经安装了chrome那就可以卸载firefox浏览器啦(说实话真觉得firefox不好用)1sudo apt-get purge firefox* 卸载一些用不到的自带软件12sudo apt-get remove thunderbird totem rhythmbox empathy brasero simple-scan gnome-mahjongg aisleriot gnome-mines cheese transmission-common gnome-orca webbrowser-app gnome-sudoku landscape-client-ui-install sudo apt-get remove onboard deja-dup 删除libreoffice，Amazon的链接1sudo apt-get remove libreoffice-common unity-webapps-common 解决Win10、Ubuntu双系统时间同步问题&emsp;&emsp;安装过Windows和Ubuntu双系统的同学们，应该都碰到过两个系统的时间并不一致这个问题。Windows的时间比Ubuntu的晚了8个小时，在网上查了相关的资料，发现在硬件时间都是一样的情况下，Ubuntu使用的是（UST），Windows使用的是（CST）。要解决该问题就要对这两个时间进行同步即可。 123sudo apt-get install ntpdatesudo ntpdate time.nist.gov sudo hwclock --localtime --systohc 然后更新系统，重启就OK了12sudo apt-get upgrade reboot 结语&emsp;&emsp;到这里Ubuntu就已经基本配置好了，至于cuda+cudnn安装和一些深度学习环境的搭建我会另外写一篇博客进行介绍。现在你已经打造完成你的个人专属Ubuntu系统，还不赶快享受一下～&emsp;&emsp;教程中难免有错误，大家可以留言告诉我，窝一定会改正的～","path":"2019/01/22/UbuntuToDo/","date":"01-22","excerpt":"","tags":[]},{"title":"MXNet学习笔记：HybridBlock类hybrid_forwar()函数解析","text":"&emsp;&emsp;MXNet中，gluon.Block类和gluon.HybridBlock类，和Pytorch中的nn.Module类一样，我们通过继承Block类和HybridBlock类可以很灵活的搭建我们自己的网络模型，这里总结一下HybridBlock类使用过程中的一些注意点。 HybridBlock类和Block类的区别&emsp;&emsp;HybridBlock类继承至Block类，所以HybridBlock类有Block类的全部方法和属性。HybridBlock同时支持符号式编程和命令式编程，HybridBlock类可以调用hybridize()方法，从而可以从命令式变为符号式，从而将动态图转化为静态图，提高模型的计算性能和移植性。下面是两者的比较： HybridBlock类 Block类 重写方法 __init__()、hybrid_forward(self, F, x, *args, **kwargs) __init__()、forwad(self,x,*args) 是否支持符号式 是 否 支持输入参数 位置式参数、关键字参数 只支持位置式参数 是否支持导出符号模型 是 否 &emsp;&emsp;可以看出HybridBlock类除了多支持符号式编程外，和Block基本没什么区别，但是注意到支持输入参数那一栏，hybrid_forward函数还支持输入关键字参数，这点也和Block不一样，下面详细分析一下hybrid_forward的调用过程。 hybrid_forward()分析&emsp;&emsp;当我们构建一个HybridBlock类后，需要重写其|__init__()、hybrid_forward()方法，而我们在源码中可以看到，当一个HybridBlock类进行forward操作时，其流程如下： &emsp;&emsp;__call__()————&gt;forward()————&gt;hybrid_forward() &emsp;&emsp;可以看出HybridBlock类是通过forward()方法中来调用hybrid_forward()。由于HybridBlock类中的forward()方法已经被重写过了，所以我们只需要重写hybrid_forward()就可以了，其中forward()函数如下： 1234567891011121314151617181920212223242526def forward(self, x, *args): \"\"\"Defines the forward computation. Arguments can be either :py:class:`NDArray` or :py:class:`Symbol`.\"\"\" if isinstance(x, NDArray): with x.context as ctx: if self._active: return self._call_cached_op(x, *args) try: params = &#123;i: j.data(ctx) for i, j in self._reg_params.items()&#125; except DeferredInitializationError: self._deferred_infer_shape(x, *args) for _, i in self.params.items(): i._finish_deferred_init() params = &#123;i: j.data(ctx) for i, j in self._reg_params.items()&#125;#!!!!!!!!!!!!!!!!!!!!!!注意这里的 params参数!!!!!!!!!!!!!!!!!!!!!!!! return self.hybrid_forward(ndarray, x, *args, **params) assert isinstance(x, Symbol), \\ \"HybridBlock requires the first argument to forward be either \" \\ \"Symbol or NDArray, but got %s\"%type(x) params = &#123;i: j.var() for i, j in self._reg_params.items()&#125; with self.name_scope(): #!!!!!!!!!!!!!!!!!!!!!!注意这里的 params参数!!!!!!!!!!!!!!!!!!!!!!!! return self.hybrid_forward(symbol, x, *args, **params) &emsp;&emsp;注意到forward()函数中会将在__init__()方法中注册的参数，会作为关键字参数传递给hybrid_forward。也就是说我们在__init__()方法中用self.params.get()或self.params.get_constant()注册的所有参数都会作为关键字参数传递给hybrid_forward。 &emsp;&emsp;下面来看一个例子，我们创建了一个gluon.HybridBlock,并且在__init__()方法中注册了一个Constant参数，我们在forward时，并没有输入anchors参数，可以看到该参数会直接传递给hybrid_forward。并且forward时会自动将所有注册的参数复制到和x相同的设备中(CPU or GPU),然后再进行运算。详细程序如下： 12345678910111213141516171819202122232425262728293031323334353637class Generator(gluon.HybridBlock): def __init__(self,alloc_size=(5, 5),**kwargs): super(SFDAnchorGenerator, self).__init__(**kwargs) anchors = self._generate_anchors(alloc_size) self._key = 'anchor_1' #注册了一个Constant参数 self.anchors = self.params.get_constant(self._key, anchors) def _generate_anchors(self,alloc_size): return nd.random.uniform(shape=alloc_size) #定义的hybrid_forward中需要有一个anchors参数 def hybrid_forward(self, F, x, anchors): print(anchors)generator = Generator()generator.initialize()x = nd.random.uniform(shape=(3,3))#可以看到，这里我并没有输入 anchors 参数，程序没有报错，而是直接打印出我们的参数 generator(x)print(generator._reg_params)Out：[[0.50962436 0.9767611 0.05571469 0.6048455 0.4511592 ] [0.7392636 0.01998767 0.03918779 0.44171092 0.28280696] [0.9795867 0.12019657 0.35944447 0.2961402 0.48089352] [0.11872771 0.68866116 0.31798318 0.8804759 0.41426298] [0.9182355 0.0641475 0.21682213 0.6924721 0.5651889 ]]&lt;NDArray 5x5 @cpu(0)&gt;&#123;'anchors': Constant sfdanchorgenerator13_anchor_1 (shape=(5, 5), dtype=&lt;class 'numpy.float32'&gt;)&#125; 总结&emsp;&emsp;需要注意到，当使用HybridBlock类时，所有在__init__()方法注册的参数，均会以关键字参数的形式传递到hybrid_forward中，所以如果我们重写的hybrid_forward函数如果没有对应参数，那么程序运行时将报错。并且HybridBlock类在前向运算时，会自动将传入的关键字参数都复制到与输入参数x相同的设备上(CPU or GPU)，灵活使用这一点，可以让我们更加灵活的搭建自己的模型。","path":"2019/01/18/mxnet-hrbrid-forward/","date":"01-18","excerpt":"","tags":[{"name":"mxnet","slug":"mxnet","permalink":"https://github.com/JJXiangJiaoJun/tags/mxnet/"}]},{"title":"MXNet使用技巧：单独设置网络中每层的学习率","text":"迁移学习 （Finetune） 中我们经常需要固定pretrained层的学习率，或者把其学习率设置比后面的网络小，这就需要我们对不同的层设置不同的学习率，这里总结一下实现设置每层学习率的方法。 使用net.collect_params(‘re’).setattr(‘lr_mult’,ratio)方法&emsp;&emsp;net.collect_params()将返回一个ParamterDict类型的变量，其中包含了网络中所有参数。其函数原型如下： 1234def collect_params(self,select=None)model.collect_params('conv1_weight|conv1_bias|fc_weight|fc_bias')model.collect_params('.*weight|.*bias') &emsp;&emsp;其中select参数可以为一个正则表达式，从而collect_params只会选择被该正则表达式匹配上的参数。我们首先把需要单独设置学习率的参数都用正则表达式匹配出来。比如说下面的ResNet50，其所有参数如下(中间省略了一些层)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960print(net.collect_params())resnet50v1 ( Parameter resnet50v1batchnorm0_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1batchnorm0_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1batchnorm0_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1batchnorm0_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1conv0_weight (shape=(64, 0, 5, 5), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm0_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm0_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm0_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm0_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_conv0_weight (shape=(64, 0, 1, 1), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm1_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm1_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm1_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm1_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_conv1_weight (shape=(64, 0, 3, 3), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm2_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm2_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm2_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm2_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_conv2_weight (shape=(256, 0, 1, 1), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1conv1_weight (shape=(256, 0, 1, 1), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1batchnorm1_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1batchnorm1_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1batchnorm1_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1batchnorm1_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm3_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm3_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm3_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm3_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_conv3_weight (shape=(64, 0, 1, 1), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm4_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm4_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm4_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm4_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_conv4_weight (shape=(64, 0, 3, 3), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm5_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm5_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm5_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm5_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_conv5_weight (shape=(256, 0, 1, 1), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer1_batchnorm6_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) ..... ..... ..... ..... ..... Parameter resnet50v1layer4_batchnorm7_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer4_conv7_weight (shape=(512, 0, 3, 3), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer4_batchnorm8_gamma (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer4_batchnorm8_beta (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer4_batchnorm8_running_mean (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer4_batchnorm8_running_var (shape=(0,), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1layer4_conv8_weight (shape=(2048, 0, 1, 1), dtype=&lt;class 'numpy.float32'&gt;) Parameter resnet50v1dense0_weight (shape=(10, 2048), dtype=float32) Parameter resnet50v1dense0_bias (shape=(10,), dtype=float32)) &emsp;&emsp;假设我们想加大最后全连接层的学习率，那么我们可以通过正则表达式将其参数选出来，net.collect_params(&#39;.*dense&#39;),结果如下：123456print(net.collect_params('.*dense'))resnet50v1 ( Parameter resnet50v1dense0_weight (shape=(10, 2048), dtype=float32) Parameter resnet50v1dense0_bias (shape=(10,), dtype=float32)) &emsp;&emsp;选完了我们需要设置的参数后，最后只要设置其lr_mult属性就行，该层的学习率为lr*lr_mult，当lr_mult=0时，那么该层参数不会更新。12trainter = mx.gluon.Trainer(net.collect_params(),'sgd',&#123;'learning_rate':0.1&#125;)net.collect_params('.*dense').setattr('lr_mult',0.1) 总结&emsp;&emsp;在net.collect_params()中，我们通过使用正则表达式匹配出需要单独设置的参数。最后再通过setattr()方法设置其学习率因子lr_mult，从而实现设置该层的学习率。所以设计网络时，我们可以通过为每层加上特定的prefix_name从而能让我们方便地用正则表达式匹配出每一层的参数。同理我们也可以通过这种方式来对不同层的参数进行单独初始化。","path":"2019/01/18/mxnet-set-learning-rate/","date":"01-18","excerpt":"","tags":[]},{"title":"python常用库总结：argparse库","text":"&emsp;&emsp;写深度学习脚本的时候需要设置各种参数，你还在手动在代码里修改？那你就out了，赶紧来学学argparse库的使用吧！ &emsp;&emsp;python中argparse库是一个命令行参数的解析工具，利用这个库可以在命令行运行脚本时，设置参数，从而灵活的设置脚本中需要的参数。下面总结一下argparse库的使用方法。创建parse对象&emsp;&emsp;argparse库为python自带的库，使用时直接import argparse就可以使用，第一步，我们要创建一个ArgumentParser对象，（设置的参数最终都会作为ArgumentParser对象的属性，调用方法如ArgumentParser.xxxxx），同时在其description参数中，我们可以输入想添加的提示信息。123import argparseparser = argparse.ArgumentParser(description='add some useful information here....') 使用add_argument()方法添加参数&emsp;&emsp;创建了ArgumentParser对象之后，就可以使用add_argument()方法来添加参数,其输入参数如下，其中[]中的参数为可选参数，作用为设置参数的一些属性。add_argument()第一个输入参数即为我们添加的参数，当前面没有-前缀，比如&#39;foo&#39;时为位置参数，当带有-前缀时，为可选参数，如--foo。位置参数和可选参数的概念这里就不解释了。12ArgumentParser.add_argument(name or flags…[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest]) 12345import argparseparser = argparse.ArgumentParser(description='arugment test')parser.add_argument('foo') #位置参数parser.add_argument('--foo2') #可选参数 &emsp;&emsp;下面先总结一下，add_argument()方法中常用的一些属性： 属性值 作用 type 指定参数存储时的类型，可为int、float、str等 action 指定参数的动作，可为store_const、store_true/store_false、append、count‘ default 指定参数的默认值 dest 指定参数的保存位置，如设置&#39;ddd&#39;，则通过parser.ddd获取该参数值 choice 指定参数的可选项，参数只能从设置值中选择 required 说明参数是否为必须的，可以设置为True\\False help 参数的说明，用户输入--help\\-h时，会显示设置的帮助信息 type&emsp;&emsp;ArgumentParser添加的参数默认存储类型为str，如果想以其他方式进行存储，就要通过设置type属性来按指定类型存储，代码和运行结果如下：123456789101112131415161718parser = argparse.ArgumentParser( formatter_class=argparse.ArgumentDefaultsHelpFormatter, description='Create an image list or \\ make a record database by reading from an image list' )parser.add_argument('--foo')parser.add_argument('--foo2', type=int)parser.add_argument('--foo3', type=float)args = parser.parse_args()print('foo is', type(args.foo))print('foo2 is', type(args.foo2))print('foo3 is', type(args.foo3))C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foo 1.1 --foo2 1 --foo3 0.1foo is &lt;class 'str'&gt;foo2 is &lt;class 'int'&gt;foo3 is &lt;class 'float'&gt; action&emsp;&emsp;action可以设置参数的动作，也就是当我们输入了该可选参数时，该参数会进行的动作。 store_const&emsp;&emsp;很简单，设置该参数的值为常量，搭配const属性使用，初始化时赋值为const中设置的值，之后程序中不允许更改参数值，这样我们可以限制一些用户的误操作，代码和运行结果如下：123456parser.add_argument('--foo',action='store_const',const='this is a constant')args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foofoo is this is a constant store_true/store_false&emsp;&emsp;store_true表示，当该参数出现时，设置该参数的值为true，否则为false。store_false则相反。123456789parser.add_argument('--foo',action='store_true')args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foofoo is TrueC:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.pyfoo is False append&emsp;&emsp;使用append后，命令行中可以为该参数赋多个值，这些值将会保存在一个列表中。123456parser.add_argument('--foo',action='append')args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foo 1,23,5foo is ['1,23,5'] count&emsp;&emsp;顾名思义，统计该参数出现的次数。123456parser.add_argument('--foo',action='count')args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foo --foo --foofoo is 3 default&emsp;&emsp;很实用的一个属性，设置参数的默认值，也就是说，当没有输入该参数时，参数默认存储default中设置的值。123456789parser.add_argument('--foo',type=str,default='default value')args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.pyfoo is default valueC:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foo not-defaultfoo is not-default dest&emsp;&emsp;指定参数的存储位置，该参数值将被存储为dest声明的变量中。123456parser.add_argument('--foo',type=str,dest='foo_dest')args = parser.parse_args()print('foo is', args.foo_dest) #注意这里的调用变量为 foo_destC:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foo dest_testfoo is dest_test choice&emsp;&emsp;用户输入的参数只能是choices里面规定的,可以限制用户的输入。使程序更加鲁棒12345678910parser.add_argument('--foo', type=int,choices=[5,6,7,8])args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foo 5foo is 5C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py --foo 1usage: arg_test.py [-h] [--foo &#123;5,6,7,8&#125;]arg_test.py: error: argument --foo: invalid choice: 1 (choose from 5, 6, 7, 8) required&emsp;&emsp;设置该参数是否为必须，如果为True则要求用户一定要输入该参数。1234567parser.add_argument('--foo', type=int,required=True)args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.pyusage: arg_test.py [-h] --foo FOOarg_test.py: error: the following arguments are required: --foo help&emsp;&emsp;设置参数帮助信息，可以在其中写入一些提示，告诉用户应该如何输入该参数，相当于一个帮助文档1234567891011121314parser.add_argument('--foo', type=int, help = 'add some helpful message here.')args = parser.parse_args()print('foo is', args.foo)C:\\Users\\lenovo\\Desktop\\arg_test&gt;py -3 arg_test.py -husage: arg_test.py [-h] [--foo FOO]Create an image list or make a record database by reading from an image listoptional arguments: -h, --help show this help message and exit --foo FOO add some helpful message here. (default: None) 用parse_args()方法解析参数&emsp;&emsp;添加完参数后，最后调用parse_args()方法，将会返回一个命名空间(namespace)，之后我们就可以愉快的调用各个设置好的参数啦~1234567891011121314def parse_arg(): parser = argparse.ArgumentParser( formatter_class=argparse.ArgumentDefaultsHelpFormatter, description='Create an image list or \\ make a record database by reading from an image list' ) parser.add_argument('--foo', type=int, help='add some helpful message here.') # parser.add_argument('--foo2', type=int) # parser.add_argument('--foo3', type=float) #namespace args = parser.parse_args() return args 总结&emsp;&emsp;总的来说，argparse库的用法还是比较傻瓜的，如果学会了，恭喜你，你再也不用在代码里面去修改参数啦~","path":"2019/01/18/python-argparse/","date":"01-18","excerpt":"","tags":[{"name":"python","slug":"python","permalink":"https://github.com/JJXiangJiaoJun/tags/python/"}]},{"title":"Faster-RCNN论文细节原理解读+代码实现gluoncv(MXNet)","text":"&emsp;&emsp;Faster-RCNN开创了基于锚框(anchors)的目标检测框架，并且提出了RPN(Region proposal network)，来生成RoI，用来取代之前的selective search方法。Faster-RCNN无论是训练/测试速度，还是物体检测的精度都超过了Fast-RCNN，并且实现了end-to-end训练。 &emsp;&emsp;从RCNN到Fast-RCNN再到Faster-RCNN，后者无疑达到了这一系列算法的巅峰，并且后来的YOLO、SSD、Mask-RCNN、RFCN等物体检测框架都是借鉴了Faster-RCNN &emsp;&emsp;Faster-RCNN作为一种two-stage的物体检测框架，流程无疑比SSD这种one-stage物体检测框架要复杂，在阅读论文，以及代码复现的过程中也理解了很多细节，在这里记录一下自己的学习过程和自己的一点体会。 背景介绍 论文主要贡献 1、网络框架 2、RPN(Region Proposal Network) 处理流程 详细步骤及代码 在feature_map上生成锚框 用conv3x3卷积进一步提取特征图 用1x1卷积层进行二分类预测以及边界框回归预测 使用预测的score和offset对锚框处理，输出Region Proposal RPN整体代码 3、对RPN输出的Region Proposal采样处理 训练过程中的Region Proposal采样 测试过程中的Region Proposal采样 Region Proposal采样代码 4、RoI Pooling层 5、后续Fast-RCNN处理 处理流程 代码 6、总结 7、题外话 背景介绍&emsp;&emsp;Fast-RCNN通过共享卷积层，极大地提升了整体的运算速度。Selective Search 反倒成为了限制计算效率的瓶颈。Faster-RCNN中使用卷积神经网络取代了Selective Search，这个网络就是Region Proposal Networks(RPN)，Faster-RCNN将所有的步骤都包含到一个完整的框架中，真正实现了端对端(end-to-end)的训练。 论文主要贡献 提出RPN，实现了端对端的训练 提出了基于anchors的物体检测方法 1、网络框架&emsp;&emsp;Faster-RCNN总体流程框图如下（点击原图查看大图），通过这个框图我们比较一下Faster-RCNN和SSD的不同： SSD中每一阶段生成的特征图，每个cell都会生成锚框，并且进行类别+边界框回归。 Faster-RCNN只对basenet提取出的特征图上生成锚框，并且对该锚框进行二分类（背景 or 有物体）+边界框回归，然后会进行NMS移除相似的结果，这样RPN最后会输出一系列region proposal，将这些region proposal区域从feature map中提取出来即为RoI，之后将会通过RoI pooling，进行真正的类别预测（判断属于哪一类）+边界框回归 &emsp;&emsp;可以看出Faster-RCNN之所以被称为two-stage，是由于需要有RPN生成region proposal这一步骤。相比来看SSD可以看做是稠密采样，它对所有生成的锚框进行了预测，而没有进行筛选。 &emsp;&emsp;RPN中还有一些细节操作，比如说采样比例的设置，如何进行预测，这个在后面的部分会详细说明。 2、RPN(Region Proposal Network)处理流程&emsp;&emsp;RPN在Faster-RCNN中作用为生成RoI，RPN的处理流程具体如下，一些细节将在之后介绍： 输入为base_net提取出来的feature map，首先在feature map上生成锚框（anchor），其中每个cell有多个锚框。 通过一个conv_3x3,stride=1,padding=1的卷积层，进一步提取特征，输出特征图的大小不变，这里称为rpn_feature。 在rpn_feature上用两个1x1卷积层进行预测输出，分别为每个锚框的二分类分数、每个锚框的坐标偏移量。 利用上面预测的分数以及偏移量，对锚框（anchor）进行非极大值抑制（NMS）操作，最终输出RoI候选区域。 详细步骤及代码在feature_map上生成锚框这一步中，会在feature_map每个cell上生成一系列不同大小和宽高比例的锚框。生成锚框的方式如下： 1. 选定一个锚框的基准大小，记为base，比如为16 2. 选定一组宽高比例(aspect ratios)，比如为【0.5、1、2】 3. 选定一组大小比例(scales)，比如为【16、32、64】 4. 那么 **每个cell** 将会生成ratios*scales个锚框，而每个锚框的形状大小的计算公式如下： $$ width_{anchor} = size_{base} \\times scale \\times \\sqrt{ 1 / ratio}$$ $$ height_{anchor} = size_{base} \\times scale \\times \\sqrt{ratio}$$ 举个例子，我们按照论文中取3种大小比例以及3种长宽比例，那么每个cell生成的锚框个数为$k=9$，而假设我们的特征图大小为$W\\times H=2400$，那么我们一共生成了$WHk$个锚框。可以看到，生成的锚框数量非常多，有大量的重复区域。RPN输出时不应该使用所有锚框，所以采用NMS 来去除大量重复的锚框，而只选择一些得分较高的锚框作为RoI输出。其实，RPN在训练时也进行了采样，这个后面具体介绍。RPN生成的锚框如下图所示： MXNet中，生成锚框的类源码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273class RPNAnchorGenerator(gluon.Block): \"\"\" @输入参数 stride:int 特征图的每个像素感受野大小，通常为原图和特征图尺寸比例 base_size:int 默认大小 ratios:int 宽高比 scales:int 大小比例 每个锚框为 width = base_size*size/sqrt(ratio) height = base_size*size*sqrt(ratio) alloc_size:(int,int) 默认的特征图大小(H,W)，以后每次生成直接索引切片 \"\"\" def __init__(self, stride, base_size, ratios, scales, alloc_size, **kwargs): super(RPNAnchorGenerator, self).__init__(**kwargs) if not base_size: raise ValueError(\"Invalid base_size: &#123;&#125;\".format(base_size)) # 防止非法输入 if not isinstance(ratios, (tuple, list)): ratios = [ratios] if not isinstance(scales, (tuple, list)): scales = [scales] # 每个像素的锚框数 self._num_depth = len(ratios) * len(scales) # 预生成锚框 anchors = self._generate_anchors(stride, base_size, ratios, scales, alloc_size) self.anchors = self.params.get_constant('anchor_', anchors) @property def num_depth(self): return self._num_depth def _generate_anchors(self, stride, base_size, ratios, scales, alloc_size): # 计算中心点坐标 px, py = (base_size - 1) * 0.5, (base_size - 1) * 0.5 base_sizes = [] for r in ratios: for s in scales: size = base_size * base_size / r ws = np.round(np.sqrt(size)) w = (ws * s - 1) * 0.5 h = (np.round(ws * r) * s - 1) * 0.5 base_sizes.append([px - w, py - h, px + w, py + h]) # 每个像素的锚框 base_sizes = np.array(base_sizes) # 下面进行偏移量的生成 width, height = alloc_size offset_x = np.arange(0, width * stride, stride) offset_y = np.arange(0, height * stride, stride) offset_x, offset_y = np.meshgrid(offset_x, offset_x) # 生成(H*W,4) offset = np.stack((offset_x.ravel(), offset_y.ravel(), offset_x.ravel(), offset_y.ravel()), axis=1) # 下面广播到每一个anchor中 (1,N,4) + (M,1,4) anchors = base_sizes.reshape((1, -1, 4)) + offset.reshape((-1, 1, 4)) anchors = anchors.reshape((1, 1, width, height, -1)).astype(np.float32) return anchors # 对原始生成的锚框进行切片操作 def forward(self, x): # 切片索引 anchors = self.anchors.value a = nd.slice_like(anchors, x * 0, axes=(2, 3)) return a.reshape((1, -1, 4)) 用conv3x3卷积进一步提取特征图&emsp;&emsp;这一步中就是RPN进一步抽取特征，生成的RPN-feature map提供给之后的类别预测和回归预测。该步骤中使用的是kernel_size=3x3，strides=1,padding=1,Activation=&#39;relu&#39;的卷积层，不改变特征图的尺寸，这也是为了之后的1x1卷积层预测时，空间位置能够一一对应，而用通道数来表示预测的类别分数和偏移量。这一步的代码很简单，就是单独的构建了一个3x3 Conv2D的卷积层。 12345# 第一个提取特征的3x3卷积self.conv1 = nn.Sequential()self.conv1.add(nn.Conv2D(channels, kernel_size=3, strides=1, padding=1, weight_initializer=weight_initializer), nn.Activation('relu')) 用1x1卷积层进行二分类预测以及边界框回归预测&emsp;&emsp;我们在第一步中生成了固定的默认锚框，这一步我们需要用两个1x1卷积层对每个锚框分别预测（1）类别分数（背景or物体）$score$（2）锚框偏移量$offset$。而这些预测值$score、offset$将用于后面的NMS操作，可以去除一些得分低，或者有大量重复区域的锚框，从而最终输出良好的Region Proposal给后面网络进行处理。 类别分数$score$，RPN中只关心是否有物体，所以是个二分类问题（背景、物体）。 锚框的坐标偏移量$offset$，一般为4个值，$\\boldsymbol\\Delta xcenter、\\boldsymbol\\Delta ycenter、\\boldsymbol\\Delta width、\\boldsymbol\\Delta height$。 &emsp;&emsp;上面介绍了，两个1x1卷积层的输入为RPN-feature map，1x1卷积并不改变特征图尺寸，我们采用通道数来表示对应cell锚框的预测值。假设输入RPN-feature map 形状为$(C，H，W)$,每个cell生成了$k$个锚框。输出的锚框分数和偏移量在空间位置上一一对应（也就是尺寸不变）。 类别分数，输出通道应为$(k\\times2,H,W)$，不同通道表示每个类别的分数 偏移量预测，输出通道应为$(k\\times4,H,W)$，不同通道表示锚框的坐标偏移量 &emsp;&emsp;代码很简单，就是添加两个卷积层并前向运算： 123456# 预测偏移量和预测类别的卷积层 # 使用sigmoid预测，减少通道数 self.score = nn.Conv2D(anchor_depth, kernel_size=1, strides=1, padding=0, weight_initializer=weight_initializer) self.loc = nn.Conv2D(anchor_depth * 4, kernel_size=1, strides=1, padding=0, weight_initializer=weight_initializer) 使用预测的score和offset对锚框处理，输出Region Proposal&emsp;&emsp;上面的步骤中，我们会对feature map的每个cell都生成多个锚框，并且预测$score、offset$，我们生成了$WHk$个锚框（大约有2W个），不难想象，大量的锚框其实都是背景，而且有着大量的重叠锚框，我们不可能将所有的锚框都当做Region Proposal输出给RoI Pooling层，提供给Fast-RCNN进行后面的进一步运算。第一个原因是会造成计算量过大，第二个原因是大量的背景框，重复的锚框是没有意义的，我们应该输出得分最高的topk个锚框。最后一步的Region Proposal具体处理过程如下： 将上一步预测的偏移量加到生成的默认锚框中，我们把这些区域称作RoI 对超出图像边界的RoI进行剪切，保证所有RoI都在原始图像内部 丢弃小于我们设定最小尺寸的锚框 根据我们预测的$score$，对RoI进行非极大值抑制操作（NMS），去除得分较低以及重复区域的RoI 最后我们选择得分为topk的RoI输出，作为最终输出的Region Proposal（比如说前2000个） &emsp;&emsp;通过这一步，我们筛选出了置信度最高的Region Proposal，也就是我们认为最有可能有物体的区域，输入到后面的Fast-RCNN网络中，进行最终的分类以及再一次的边界框回归预测。MXNet GluonCV 中生成Region Proposal的类源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788class RPNProposal(gluon.Block): \"\"\" @:parameter ------------------ clip : float 如果提供，将bbox剪切到这个值 num_thresh : float nms的阈值，用于去除重复的框 train_pre_nms : int 训练时对前 train_pre_nms 进行 NMS操作 train_post_nms : int 训练时进行NMS后，返回前 train_post_nms 个region proposal test_pre_nms : int 测试时对前 test_pre_nms 进行 NMS操作 test_post_nms : int 测试时进行NMS后，返回前 test_post_nms 个region proposal min_size : int 小于 min_size 的 proposal将会被舍弃 stds : tuple of int 计算偏移量用的标准差 \"\"\" def __init__(self, clip, nms_thresh, train_pre_nms, train_post_nms, test_pre_nms, test_post_nms, min_size, stds, **kwargs): super(RPNProposal, self).__init__(**kwargs) self._clip = clip self._nms_thresh = nms_thresh self._train_pre_nms = train_pre_nms self._train_post_nms = train_post_nms self._test_pre_nms = test_pre_nms self._test_post_nms = test_post_nms self._min_size = min_size self._bbox_decoder = NormalizedBoxCenterDecoder(stds=stds, clip=clip) self._cliper = BBoxClipToImage() self._bbox_tocenter = BBoxCornerToCenter(axis=-1, split=False) \"\"\" @:parameter scores : （B,N,1) 通过RPN预测的得分输出(sigmoid之后) (0,1) offsets : ndarray (B,N,4) 通过RPN预测的锚框偏移量 anchors : ndarray (B,N,4) 生成的默认锚框，坐标编码方式为 Corner img : ndarray (B,C,H,W) 图像的张量，用来剪切锚框 @:returns \"\"\" def forward(self, scores, offsets, anchors, img): # 训练和预测的处理流程不同 if autograd.is_training(): pre_nms = self._train_pre_nms post_nms = self._train_post_nms else: pre_nms = self._test_pre_nms post_nms = self._test_post_nms with autograd.pause(): # 将预测的偏移量加到anchors中 rois = self._bbox_decoder(offsets, self._bbox_tocenter(anchors)) rois = self._cliper(rois, img) # 下面将所有尺寸小于设定最小值的ROI去除 x_min, y_min, x_max, y_max = nd.split(rois, num_outputs=4, axis=-1) width = x_max - x_min height = y_max - y_min invalid_mask = (width &lt; self._min_size) + (height &lt; self._min_size) # 将对应位置的score 设为-1 scores = nd.where(invalid_mask, nd.ones_like(scores) * -1, scores) invalid_mask = nd.repeat(invalid_mask, repeats=4, axis=-1) rois = nd.where(invalid_mask, nd.ones_like(rois) * -1, rois) # 下面进行NMS操作 pre = nd.concat(scores, rois, dim=-1) pre = nd.contrib.box_nms(pre, overlap_thresh=self._nms_thresh, topk=pre_nms, coord_start=1, score_index=0, id_index=-1, force_suppress=True) # 下面进行采样 result = nd.slice_axis(pre,axis=1, begin=0, end=post_nms) rpn_score = nd.slice_axis(result, axis=-1, begin=0, end=1) rpn_bbox = nd.slice_axis(result, axis=-1, begin=1, end=None) return rpn_score, rpn_bbox &emsp;&emsp;RPN最终输出的Region Proposal 如图所示，去除了大量的重复锚框，和得分低的背景区域： RPN整体代码&emsp;&emsp;RPN的处理流程如上所述，下面是RPN层的整体代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# 定义RPN网络# RPN网络输出应为一系列 region proposal 默认为 2000个class RPN(nn.Block): \"\"\" @输入参数 channels : int 卷积层的输出通道 stride:int 特征图的每个像素感受野大小，通常为原图和特征图尺寸比例 base_size:int 默认大小 ratios:int 宽高比 scales:int 大小比例 每个锚框为 width = base_size*size/sqrt(ratio) height = base_size*size*sqrt(ratio) alloc_size:(int,int) 默认的特征图大小(H,W)，以后每次生成直接索引切片 clip : float 如果设置则将边界框剪切到该值 nms_thresh : float 非极大值抑制的阈值 train_pre_nms : int 训练时对前 train_pre_nms 进行 NMS操作 train_post_nms : int 训练时进行NMS后，返回前 train_post_nms 个region proposal test_pre_nms : int 测试时对前 test_pre_nms 进行 NMS操作 test_post_nms : int 测试时进行NMS后，返回前 test_post_nms 个region proposal min_size : int 小于 min_size 的 proposal将会被舍弃 \"\"\" def __init__(self, channels, stride, base_size, ratios, scales, alloc_size, clip, nms_thresh, train_pre_nms, train_post_nms, test_pre_nms, test_post_nms , min_size, **kwargs): super(RPN, self).__init__(**kwargs) weight_initializer = mx.init.Normal(sigma=0.01) # 锚框生成器 self._anchor_generator = RPNAnchorGenerator(stride, base_size, ratios, scales, alloc_size) anchor_depth = self._anchor_generator.num_depth self._rpn_proposal = RPNProposal(clip, nms_thresh, train_pre_nms, train_post_nms, test_pre_nms, test_post_nms, min_size, stds=(1., 1., 1., 1.)) # 第一个提取特征的3x3卷积 self.conv1 = nn.Sequential() self.conv1.add(nn.Conv2D(channels, kernel_size=3, strides=1, padding=1, weight_initializer=weight_initializer), nn.Activation('relu')) # 预测偏移量和预测类别的卷积层 # 使用sigmoid预测，减少通道数 self.score = nn.Conv2D(anchor_depth, kernel_size=1, strides=1, padding=0, weight_initializer=weight_initializer) self.loc = nn.Conv2D(anchor_depth * 4, kernel_size=1, strides=1, padding=0, weight_initializer=weight_initializer) # 前向运算函数 def forward(self, x, img): \"\"\" 产生锚框，并且对每个锚框进行二分类，以及回归预测 ************************ 注意，这一阶段只是进行了粗采样，在RCNN中还要进行一次采样 @:parameter ------------- x : (B,C,H,W） 由basenet提取出的特征图 img : (B,C,H,W） 图像tensor，用来剪切超出边框的锚框 @:returns ----------------- (1)训练阶段 rpn_score : ndarray (B,train_post_nms,1) 输出的region proposal 分数 (用来给RCNN采样) rpn_box : ndarray (B,train_post_nms,4) 输出的region proposal坐标 Corner raw_score : ndarray (B,N,1) 卷积层的原始输出，用来训练RPN rpn_bbox_pred : ndarray (B,N,4) 卷积层的原始输出，用来训练RPN anchors : ndarray (1,N,4) 生成的锚框 (2)预测阶段 rpn_score : ndarray (B,train_post_nms,1) 输出的region proposal 分数 (用来给RCNN采样) rpn_box : ndarray (B,train_post_nms,4) 输出的region proposal坐标 Corner \"\"\" anchors = self._anchor_generator(x) # 提取特征 feat = self.conv1(x) # 预测 raw_score = self.score(feat) raw_score = raw_score.transpose((0, 2, 3, 1)).reshape(0, -1, 1) rpn_scores = mx.nd.sigmoid(mx.nd.stop_gradient(raw_score)) rpn_bbox_pred = self.loc(feat) rpn_bbox_pred = rpn_bbox_pred.transpose((0, 2, 3, 1)).reshape(0, -1, 4) # 下面生成region proposal rpn_score, rpn_box = self._rpn_proposal( rpn_scores, mx.nd.stop_gradient(rpn_bbox_pred), anchors,img) # 处于训练阶段 if autograd.is_training(): # raw_score, rpn_bbox_pred 用于 RPN 的训练 return rpn_score, rpn_box, raw_score, rpn_bbox_pred, anchors # 处于预测阶段 return rpn_score, rpn_box 3、对RPN输出的Region Proposal采样处理&emsp;&emsp;上面说道通过RPN层后，我们进行了粗采样，输出了大约2000个Region Proposal，然而我们并不会将这个2000个Region Proposal全部送入RoI Pooling中进行计算，这样效率很低、计算很慢。论文作者对这些Region Proposal进行了采样处理，只采样了一小部分的Region Proposal送入之后的网络运算，而且训练过程的采样和预测过程的采样是不一样的。下面详细介绍一下处理流程。 训练过程中的Region Proposal采样 &emsp;&emsp;训练过程的采样在Fast-RCNN论文中有提到，由于要考虑训练过程中**正负样本均衡**的问题，最终输出了128个Region Proposal，其中正样本的比例为0.25。正负样本的定义如下： * 如果一个Region Proposal与任意一个ground truth的 IoU 大于设定阈值（默认为0.5），那么标记其为正样本，否则为负样本。 * &emsp;&emsp;将所有Region Proposal打上标记后，进行随机采样，其中采样正样本的比例为0.25，其余的为负样本。最终采样输出128个Region Proposal，送入之后的网络进行处理计算。 ## 测试过程中的Region Proposal采样 &emsp;&emsp;测试过程中的采样很简单，直接采样Region Proposal中，$scores$为前topk个（比如300）的样本，目的就是提取最有可能为物体的区域输入到后面的网络了。 Region Proposal采样代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144class RCNNTargetSampler(gluon.Block): \"\"\" @:parameter ------------ num_images : int 每个batch的图片数，目前仅支持1 num_inputs : int 输入的RoI 数量 num_samples : int 输出的采样 RoI 数量 pos_thresh : float 正类样本阈值 pos_ratio : float 采样正样本的比例 max_gt_box : int \"\"\" def __init__(self, num_images, num_inputs, num_samples, pos_thresh, pos_ratio, max_gt_box, **kwargs): super(RCNNTargetSampler, self).__init__(**kwargs) self._num_images = num_images self._num_inputs = num_inputs self._num_samples = num_samples self._pos_thresh = pos_thresh self._pos_ratios = pos_ratio self._max_pos = int(np.round(num_samples * pos_ratio)) self._max_gt_box = max_gt_box def forward(self, rois, scores, gt_bboxes): \"\"\" @:parameter ----------- rois : ndarray (B,self._num_inputs,4) RPN输出的roi区域坐标，Corner scores : ndarray (B,self._num_inputs,1) RPN输出的roi区域分数，(0,1) -1表示忽略 gt_bboxes:ndarray (B,M,4) ground truth box 坐标 @:returns ----------- new_rois : ndarray (B,self._num_samples,4) 采样后的RoI区域 new_samples : ndarray (B,self._num_samples,1) 采样后RoI区域的标签 1:pos -1:neg 0:ignore new_matches : ndarray (B,self._num_samples,1) 采样后的RoI匹配的锚框编号 [0,M) \"\"\" new_rois, new_samples, new_matches = [], [], [] # 对每个batch分别进行处理 for i in range(self._num_images): roi = nd.squeeze(nd.slice_axis(rois, axis=0, begin=i, end=i + 1), axis=0) score = nd.squeeze(nd.slice_axis(scores, axis=0, begin=i, end=i + 1), axis=0) gt_bbox = nd.squeeze(nd.slice_axis(gt_bboxes, axis=0, begin=i, end=i + 1), axis=0) # 将ground truth的分数设置为1 形状为(M,1) gt_score = nd.ones_like(nd.sum(gt_bbox, axis=-1, keepdims=True)) # 将ground truth 和 roi 拼接 (N+M,4) (N+m,1) roi = nd.concat(roi, gt_bbox, dim=0) score = nd.concat(score, gt_score, dim=0).squeeze(axis=-1) # 计算iou (N+M,M) iou = nd.contrib.box_iou(roi, gt_bbox, format='corner') # (N+M,) iou_max = nd.max(iou, axis=-1) # (N+M,) 与哪个ground truth 匹配 iou_argmax = nd.argmax(iou, axis=-1) # 将所有的标记为 2 neg mask = nd.ones_like(iou_argmax) * 2 # 标记ignore 为 0 mask = nd.where(score &lt; 0, nd.zeros_like(mask), mask) # 将正类标记为 3 pos pos_idx = (iou_max &gt;= self._pos_thresh) mask = nd.where(pos_idx, nd.ones_like(mask) * 3, mask) # 下面进行shuffle操作 rand = nd.random.uniform(0, 1, shape=(self._num_inputs + self._max_gt_box,)) # 取前面 N+M 个 对mask 做shuffle操作 rand = nd.slice_like(rand, mask) # shuffle 操作后的 index index = nd.argsort(rand) # 将三个结果进行shuffle mask = nd.take(mask, index) iou_argmax = nd.take(iou_argmax, index) # 下面进行采样 # 排序 3:pos 2:neg 0:ignore order = nd.argsort(mask, is_ascend=False) # 取topk个作为正例 topk = nd.slice_axis(order, axis=0, begin=0, end=self._max_pos) # 下面取出相对应的值 pos_indices = nd.take(index, topk) pos_samples = nd.take(mask, topk) pos_matches = nd.take(iou_argmax, topk) # 下面将原来的标签改了 pos_samples = nd.where(pos_samples == 3, nd.ones_like(pos_samples), pos_samples) pos_samples = nd.where(pos_samples == 2, nd.ones_like(pos_samples) * -1, pos_samples) index = nd.slice_axis(index, axis=0, begin=self._max_pos, end=None) mask = nd.slice_axis(mask, axis=0, begin=self._max_pos, end=None) iou_argmax = nd.slice_axis(iou_argmax, axis=0, begin=self._max_pos, end=None) # 对负样本进行采样 # neg 2----&gt;4 mask = nd.where(mask == 2, nd.ones_like(mask) * 4, mask) order = nd.argsort(mask, is_ascend=False) num_neg = self._num_samples - self._max_pos bottomk = nd.slice_axis(order, axis=0, begin=0, end=num_neg) neg_indices = nd.take(index, bottomk) neg_samples = nd.take(mask, bottomk) neg_matches = nd.take(iou_argmax, topk) neg_samples = nd.where(neg_samples == 3, nd.ones_like(neg_samples), neg_samples) neg_samples = nd.where(neg_samples == 4, nd.ones_like(neg_samples) * -1, neg_samples) # 输出 new_idx = nd.concat(pos_indices, neg_indices, dim=0) new_sample = nd.concat(pos_samples, neg_samples, dim=0) new_match = nd.concat(pos_matches, neg_matches, dim=0) new_rois.append(roi.take(new_idx)) new_samples.append(new_sample) new_matches.append(new_match) new_rois = nd.stack(*new_rois, axis=0) new_samples = nd.stack(*new_samples, axis=0) new_matches = nd.stack(*new_matches, axis=0) return new_rois, new_samples, new_matches 4、RoI Pooling层&emsp;&emsp;通过上一步的采样后，我们得到了一堆没有class score的Region Proposal，这些Region Proposal是对应于我们第一步base net 提取出来 feature map上的区域。可以从网络图中看到，我们最终将Region Proposal又输出回我们feature map，我们可以将RPN看做是一个额外的中间过程，这也是Faster-RCNN被称为two-stage的原因。由于输出的Region Proposal大小并不一致，而Fast-RCNN最后为全连接层，需要输出固定尺寸的特征，所以RoI Pooling层的作用就是将这些大小不同的Region Proposal，映射输出为统一大小的特征图。比如我设置RoI Pooling层的输出大小为(14,14)，那么无论输入的特征图尺寸是什么，输出的特征图均为（14,14）。 &emsp;&emsp;代码的话直接使用nd.ROIPooling()就能实现了。 5、后续Fast-RCNN处理处理流程&emsp;&emsp;到了这一步我们的处理已经到了尾声了，我们通过RoI Pooling已经得到了固定尺寸的feature map，最后一步就是用Fast-RCNN，进行预测类别分数以及边界框的回归。具体的处理流程如下： 使用卷积层再提取一次特征 进行全局池化，将特征图尺寸变为(channel，1，1) 通过两个不同的全连接层，分别预测类别分数和进行坐标回归 类别预测全连接层有num_classes+1个神经元，其中包括所有类别和背景 坐标回归全连接层有4*num_classes个神经元，它会为每一个类别预测4个坐标回归值$\\boldsymbol\\Delta xcenter、\\boldsymbol\\Delta ycenter、\\boldsymbol\\Delta width、\\boldsymbol\\Delta height$ &emsp;&emsp;最后如果是测试的话，那么将输入的Region Proposal加上我们预测的偏移量，然后根据预测得分再进行一次NMS操作，那么就可以得到我们最终输出的物体框。并且我们可以设定一个阈值（如0.5），得分大于阈值的物体框我们才进行输出。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157class FasterRCNN(RCNN): \"\"\" @:parameter ------------- \"\"\" def __init__(self, features, top_features, classes, short=600, max_size=1000, train_patterns=None, nms_thresh=0.3, nms_topk=400, post_nms=100, roi_mode='align', roi_size=(14, 14), stride=16, clip=None, rpn_channel=1024, base_size=16, scales=(8, 16, 32), ratios=(0.5, 1, 2), alloc_size=(128, 128), rpn_nms_thresh=0.7, rpn_train_pre_nms=12000, rpn_train_post_nms=2000, rpn_test_pre_nms=6000, rpn_test_post_nms=300, rpn_min_size=16, num_sample=128, pos_iou_thresh=0.5, pos_ratio=0.25, max_num_gt=300, **kwargs): super(FasterRCNN, self).__init__( features=features, top_features=top_features, classes=classes, short=short, max_size=max_size, train_patterns=train_patterns, nms_thresh=nms_thresh, nms_topk=nms_topk, post_nms=post_nms, roi_mode=roi_mode, roi_size=roi_size, stride=stride, clip=clip, **kwargs) self._max_batch = 1 # 最大支持batch=1 self._num_sample = num_sample self._rpn_test_post_nms = rpn_test_post_nms self._target_generator = &#123;RCNNTargetGenerator(self.num_class)&#125; with self.name_scope(): # Faster-RCNN的RPN self.rpn = RPN( channels=rpn_channel, stride=stride, base_size=base_size, scales=scales, ratios=ratios, alloc_size=alloc_size, clip=clip, nms_thresh=rpn_nms_thresh, train_pre_nms=rpn_train_pre_nms, train_post_nms=rpn_train_post_nms, test_pre_nms=rpn_test_pre_nms, test_post_nms=rpn_test_post_nms, min_size=rpn_min_size) # 用来给训练时Region Proposal采样，正负样本比例为0.25 self.sampler = RCNNTargetSampler( num_images=self._max_batch, num_inputs=rpn_train_post_nms, num_samples=self._num_sample, pos_thresh=pos_iou_thresh, pos_ratio=pos_ratio, max_gt_box=max_num_gt) @property def target_generator(self): return list(self._target_generator)[0] def forward(self, x, gt_boxes=None): \"\"\" :param x: ndarray (B,C,H,W) :return: \"\"\" def _split_box(x, num_outputs, axis, squeeze_axis=False): a = nd.split(x, axis=axis, num_outputs=num_outputs, squeeze_axis=squeeze_axis) if not isinstance(a, (list, tuple)): return [a] return a # 首先用basenet抽取特征 feat = self.features(x) # 输入RPN网络 if autograd.is_training(): # 训练过程 rpn_score, rpn_box, raw_rpn_score, raw_rpn_box, anchors = self.rpn(feat, nd.zeros_like(x)) # 采样输出 rpn_box, samples, matches = self.sampler(rpn_box, rpn_score, gt_boxes) else: # 预测过程 # output shape (B,N,4) _, rpn_box = self.rpn(feat, x) # 对输出的Region Proposal 进行采样 # 输出送到后面运算的RoI # rois shape = (B,self._num_sampler,4), num_roi = self._num_sample if autograd.is_training() else self._rpn_test_post_nms # 将rois变为2D，加上batch_index with autograd.pause(): roi_batchid = nd.arange(0, self._max_batch, repeat=num_roi) rpn_roi = nd.concat(*[roi_batchid.reshape((-1, 1)), rpn_box.reshape((-1, 4))], dim=-1) rpn_roi = nd.stop_gradient(rpn_roi) # RoI Pooling 层 if self._roi_mode == 'pool': # (Batch*num_roi,channel,H,W) pool_feat = nd.ROIPooling(feat, rpn_roi, self._roi_size, 1 / self._stride) elif self._roi_mode == 'align': pool_feat = nd.contrib.ROIAlign(feat, rpn_roi, self._roi_size, 1 / self._stride, sample_ratio=2) else: raise ValueError(\"Invalid roi mode: &#123;&#125;\".format(self._roi_mode)) top_feat = self.top_features(pool_feat) avg_feat = self.global_avg_pool(top_feat) # 类别预测，回归预测 # output shape (B*num_roi,(num_cls+1)) -&gt; (B,N,C) cls_pred = self.class_predictor(avg_feat) # output shape (B*num_roi,(num_cls)*4) -&gt; (B,N,C,4) box_pred = self.bbox_predictor(avg_feat) cls_pred = cls_pred.reshape((self._max_batch, num_roi, self.num_class + 1)) box_pred = box_pred.reshape((self._max_batch, num_roi, self.num_class, 4)) # 训练过程 if autograd.is_training(): return (cls_pred, box_pred, rpn_box, samples, matches, raw_rpn_score, raw_rpn_box, anchors) # 预测过程 # 还要进行的步骤，将预测的类别和预测的偏移量加到输入的RoI中 else: # 直接输出所有类别的信息 # cls_id (B,N,C) scores(B,N,C) cls_ids, scores = self.cls_decoder(nd.softmax(cls_pred, axis=-1)) # 将所有的C调换到第一维 # (B,N,C) -----&gt; (B,N,C,1) -------&gt; (B,C,N,1) cls_ids = cls_ids.transpose((0, 2, 1)).reshape((0, 0, 0, 1)) # (B,N,C) -----&gt; (B,N,C,1) -------&gt; (B,C,N,1) scores = scores.transpose((0, 2, 1)).reshape((0, 0, 0, 1)) # (B,N,C,4) -----&gt; (B,C,N,4), box_pred = box_pred.transpose((0, 2, 1, 3)) rpn_boxes = _split_box(rpn_box, num_outputs=self._max_batch, axis=0, squeeze_axis=False) cls_ids = _split_box(cls_ids, num_outputs=self._max_batch, axis=0, squeeze_axis=True) scores = _split_box(scores, num_outputs=self._max_batch, axis=0, squeeze_axis=True) box_preds = _split_box(box_pred, num_outputs=self._max_batch, axis=0, squeeze_axis=True) results = [] # 对每个batch分别进行decoder nms for cls_id, score, box_pred, rpn_box in zip(cls_ids, scores, box_preds, rpn_boxes): # box_pred(C,N,4) rpn_box(1,N,4) box (C,N,4) box = self.box_decoder(box_pred, self.box_to_center(rpn_box)) # cls_id (C,N,1) score (C,N,1) box (C,N,4) # result (C,N,6) res = nd.concat(*[cls_id, score, box], dim=-1) # nms操作 (C,self.nms_topk,6) res = nd.contrib.box_nms(res, overlap_thresh=self.nms_thresh, valid_thresh=0.0001, topk=self.nms_topk, coord_start=2, score_index=1, id_index=0, force_suppress=True) res = res.reshape((-3, 0)) results.append(res) results = nd.stack(*results, axis=0) ids = nd.slice_axis(results, axis=-1, begin=0, end=1) scores = nd.slice_axis(results, axis=-1, begin=1, end=2) bboxes = nd.slice_axis(results, axis=-1, begin=2, end=6) # 输出为score,bbox return ids, scores, bboxes 6、总结&emsp;&emsp;总的来说Faster-RCNN主要的改进地方在于用RPN来生成候选区域，使整个预测，训练过程都能用深度学习的方法完成。Faster-RCNN达到了这一系列算法的巅峰，并且在论文中提出的基于anchor的物体检测方法，更是被之后的state-of-the-art的框架广泛采用。Faster-RCNN 在 COCO和PASCAL数据集上都取得了当时最好的成绩，感兴趣的话，具体数据在论文中都有详细提到。Faster-RCNN比SSD处理流程要复杂许多，其中还涉及到非常多的细节，例如如何对anchor进行标记，如何对整个网络进行训练等等，这些我会另外写一篇博客来记录Faster-RCNN的训练过程。 7、题外话&emsp;&emsp;Faster-RCNN我也是学习了很久了，从读论文到看源码，最深的一个感受就是“纸上得来终觉浅，绝知此事要躬行”。论文上始终都是宏观的东西，看完之后觉得自己似乎是懂了，但是当写代码时，才会发现有许多许多问题。我想只有当把代码和论文同时完全理解，才能算真正的看懂了吧。现在我的水平还完全不够，还停留在能看懂，稍微改改能用的阶段，如果是一篇新论文，要自己从零开始复现，目前的我还做不到，不过坚持下去多看多想多学多写，每天进步一点点，我想在毕业之前应该能达到我想要的目标吧~ &emsp;&emsp;学习过程中还有一个很深的体会就是多看底层源码，我就是通过看GluonCV中Faster-RCNN源码才理解了论文中的许多细节，总之多向这些优秀的代码学习吧，特别是深度学习框架的一些高级API使用，只有看了源码才会想到，原来代码还可以这样编~ &emsp;&emsp;以上Faster-RCNN都是我的个人浅薄理解，欢迎大家指出我存在的问题~","path":"2019/01/18/faster-rcnn/","date":"01-18","excerpt":"","tags":[{"name":"mxnet","slug":"mxnet","permalink":"https://github.com/JJXiangJiaoJun/tags/mxnet/"},{"name":"faster_rcnn","slug":"faster-rcnn","permalink":"https://github.com/JJXiangJiaoJun/tags/faster-rcnn/"}]},{"title":"C++ Traits简介（OpenCV中DataType类分析）","text":"&emsp;&emsp;如果你能看完这篇博客，并且能初步了解C++中Trait编程技巧的用法，那么恭喜你！你对于C++的理解已经比别人更深一层次了~O(∩_∩)O。不过Traits技巧我也只是略懂皮毛，这篇博客主要也是说说我的浅显理解，如有错误，敬请谅解 ~ Traits简介 一个简单的例子 OpenCV中的DataType类 总结 Traits简介&emsp;&emsp;初次接触到 _Traits_ 是在学习OpenCV的过程中，OpenCV中有一个DataType类，当时很不理解这个类到底是做什么用的，下面是关于它的描述： When OpenCV library functions need to communicate the concept of a particular data type, they do so by creating an object of type cv::DataType&lt;&gt;. cv::DataType&lt;&gt; itself is a template, and so the actual objects passed around are specializations of this template. This is an example of what in C++ are generally called traits.This allows the cv::DataType&lt;&gt; object to contain both runtime information about the type, as well as typedef statements in its own definition that allow it to refer to the same type at compile time. &emsp;&emsp;我对这段话的理解是，cv::DataType&lt;&gt;是一个模板类，当OpenCV中库函数需要传递特定数据类型的某些概念信息时，那么就可以通过创建cv::DataType&lt;&gt; 类型的对象来实现。我们使用的不是它本身，而是它特定的实例化对象。C++中这种用法叫做Traits。 &emsp;&emsp;说实话，第一次看这一篇章，我也没有看懂cv::DataType&lt;&gt;到底是做什么用的，不过下面讲解一下Traits后，就能明白上面说意思了。 &emsp;&emsp;简单来说，如果我们封装了一个算法，这个算法可能会由于输入数据类型的不同导致算法内部处理逻辑的不同（比如说传入的是int类型我们做一种操作，而传入的是double类型我们将进行另外一种操作），而我们并不想由于这种原因修改算法的封装时，Traits就派上用场了，它可以帮我们很方便的实现功能，而又不破坏函数的封装。&emsp;&emsp;Traits在开发者中运用特别多，主要也就是为了解决用户的负担，让一些复杂逻辑处理留给开发者来做，用户只需要根据要求调用API函数即可。 一个简单的例子&emsp;&emsp;比如我们编写了一个模板类，其中有一个sort函数，我们有以下需求： 输入类型参数 操作 int类型 将所有的数加5后排序 double类型 将所有的数减去2.0后排序 123456789template&lt;typename _Tp&gt; class algorithm&#123;public: _Tp* sort(_Tp *array) &#123; //do something ...... &#125;&#125;; &emsp;&emsp;程序在编译时，并不知道模板T是什么类型的参数，这个参数需要在程序运行实例化创建一个对象时，才具体知道T是什么。所以我们不能直接通过if T is xxx，这种方式来处理我们的逻辑。 &emsp;&emsp;一种naive的处理方式为再为类模板添加一个参数，指明传入的参数是什么类型。 12345678910#define TYPE_INT 1#define TYPE_DOUBLE 2#define ..........template&lt;typename _Tp,int type&gt; class algorithm&#123;public: //do something ...... _Tp* sort(_Tp *array)&#125;; &emsp;&emsp;我们实例化时方式如下algorithm&lt;int,1&gt;，这样我们的确能实现功能，不过这样就变成了，由用户来指明输入的参数类型。当我们可能有许多的类型需要判断时，我们的输入模板可能会变成这样template&lt;typename _Tp,int type,bool isxxx,int size............&gt;，这样严重的破坏了函数的封装性，用户也并不关心sort()函数内部实现的逻辑，这种判断工作应该交给开发者完成。 &emsp;&emsp;这时候就轮到我们的Traits技巧登场啦，看如下代码所示： 1234567891011121314151617181920212223242526#define TYPE_DEFAULT 0#define TYPE_INT 1#define TYPE_DOUBLE 2#define ..........template&lt;typename _Tp&gt; class traits&#123;public: static int data_type = TYPE_DEFAULT;&#125;;template&lt;&gt; class traits&lt;int&gt;&#123;public: static int data_type = TYPE_INT;&#125;;template&lt;&gt; class traits&lt;double&gt;&#123;public: static int data_type = TYPE_DOUBLE;&#125;; &emsp;&emsp;上面我们定义的模板类traits都是同一个类，其中第一个没有给定类型参数的为默认的实例化模板，但是后面两个给定了参数（int、double）的为显示具体化，显示具体化优先于常规模板，简单来说也就是如果给定了一个显示具体化的模板，当输入类型为该类型时，它会执行显示具体化模板中的代码，上面traits&lt;int&gt;::data_type = TPYE_INT，traits&lt;double&gt;::data_type = TYPE_DOUBLE。有了traits我们上面定义的函数中可以这样写，而不用修改函数的任何封装。 1234567891011121314151617181920#define TYPE_INT 1#define TYPE_DOUBLE 2#define ..........template&lt;typename _Tp,int type&gt; class algorithm&#123;public: //do something ...... _Tp* sort(_Tp *array) &#123; switch(traits&lt;_Tp&gt;::data_type) &#123; case TYPE_INT: //do something case TYPE_DOUBLE: //do something ........ &#125; &#125;&#125;; &emsp;&emsp;通过上面这种技术，我们将程序在运行时候的信息和程序编译时候的信息绑定在了一起，这也就是OpenCV中的描述。我们可以很巧妙的判断，一个模板类输入参数类型到底是什么。C++ STL库中的泛型算法就广泛用到了这种编程技巧。 This allows the cv::DataType&lt;&gt; object to contain both runtime information about the type, as well as typedef statements in its own definition that allow it to refer to the same type at compile time. OpenCV中的DataType类&emsp;&emsp;Datatype类正是实现了Traits的功能，我们知道OpenCV中的库函数都支持我们输入各种类型的数据，即使数据类型与不是OpenCV的原生数据类型，比如说下面代码，我们通过STL中的复数类型来初始化我们的矩阵，而其并不是OpenCV原生数据类型，这是由于OpenCV定义了DataType&lt;std::complex&lt;_Tp&gt; &gt;这样的一个显示具体化模板。1Mat B = Mat_&lt;std::complex&lt;double&gt; &gt;(3, 3); &emsp;&emsp;OpenCV中DataType类的定义如下,我们知道数据类型不同时枚举体中的各个属性值都不一样，比如说uchar类型的数据channels=1，而Point类型的channels=2。&emsp;&emsp;我们在调用OpenCV库函数时，不同数据类型的处理过程可能会不一样，所以OpenCV为每一个数据类型都定义其显示具体化模板，通过typedef语句，我们可以很方便的通过调用DataType&lt;_Tp&gt;::value_type来获取当前实例化模板的数据类型。&emsp;&emsp;OpenCV中的库函数也是通过这样对不同的数据类型进行不同的处理。而我们用户调用时，只需要给定输入类型就OK了，并不需要我们输入一些额外的参数。 1234567891011121314template&lt;typename _Tp&gt; class DataType&#123;public: typedef _Tp value_type; typedef value_type work_type; typedef value_type channel_type; typedef value_type vec_type; enum &#123; generic_type = 1, depth = -1, channels = 1, fmt = 0, type = CV_MAKETYPE(depth, channels) &#125;;&#125;; &emsp;&emsp;OpenCV中一些显示具体化的类模板如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051template&lt;&gt; class DataType&lt;bool&gt;&#123;public: typedef bool value_type; typedef int work_type; typedef value_type channel_type; typedef value_type vec_type; enum &#123; generic_type = 0, depth = CV_8U, channels = 1, fmt = (int)'u', type = CV_MAKETYPE(depth, channels) &#125;;&#125;;template&lt;typename _Tp&gt; class DataType&lt; Complex&lt;_Tp&gt; &gt;&#123;public: typedef Complex&lt;_Tp&gt; value_type; typedef value_type work_type; typedef _Tp channel_type; enum &#123; generic_type = 0, depth = DataType&lt;channel_type&gt;::depth, channels = 2, fmt = DataType&lt;channel_type&gt;::fmt + ((channels - 1) &lt;&lt; 8), type = CV_MAKETYPE(depth, channels) &#125;; typedef Vec&lt;channel_type, channels&gt; vec_type;&#125;;template&lt;&gt; class DataType&lt;Range&gt;&#123;public: typedef Range value_type; typedef value_type work_type; typedef int channel_type; enum &#123; generic_type = 0, depth = DataType&lt;channel_type&gt;::depth, channels = 2, fmt = DataType&lt;channel_type&gt;::fmt + ((channels - 1) &lt;&lt; 8), type = CV_MAKETYPE(depth, channels) &#125;; typedef Vec&lt;channel_type, channels&gt; vec_type;&#125;; &emsp;&emsp;最后我们回头看一下最先说的OpenCV中对于DataType数据结构的描述，我们的确是使用了其各种类型实例化对象，也正如OpenCV中描述的那样，我们将编译时的类型信息转换为了与OpenCV兼容的数据类型标识符。 123456// allocates a 30x40 floating-point matrixMat A(30, 40, DataType&lt;float&gt;::type);Mat B = Mat_&lt;std::complex&lt;double&gt; &gt;(3, 3);// the statement below will print 6, 2 , that is depth == CV_64F, channels == 2cout &lt;&lt; B.depth() &lt;&lt; \", \" &lt;&lt; B.channels() &lt;&lt; endl; 总结&emsp;&emsp;C++中的模板用法真是博大精深，正是有了Traits技巧，STL中的算法才能够支持各种数据类型，那么如果你想为你的程序写一个通用的算法，而并不想因为要处理不同的输入数据类型而破坏函数的封装时，不妨试试Traits吧！ ==参考资料== 【C++模版之旅】神奇的Traits我对C++ Traits编程技法的一点点理解","path":"2019/01/18/traits/","date":"01-18","excerpt":"","tags":[]},{"title":"LeetCode 第四题 Median of Two Sorted Arrays（计算两个有序数组的中位数）","text":"&emsp;&emsp;这题要求的时间复杂度为$O(log(m+n))$，这一点感觉挺难想的，我打算是用递归将一个数组分割然后插入到另一个数组中，不过调了很久都没过，题解中的解法很巧妙，而且可以用在找两个有序数组中第$k_{th}$元素。 &emsp;&emsp;下面看一下中位数的作用： 将一个集合分成连个长度相等的子集合，其中一个子集合中的元素全部大于另一个子集合中的元素 &emsp;&emsp;随机将一个长度为$m$的数组分成两份，我们一共有$m+1$种分法。$i=0\\to m$ left_A right_A $A[0],A[1],…..,A[i-1]$ $A[i],A[i+1],…..,A[m-1]$ 其中有$len(left_A)=i，len(right_A)=m-i$，当$i=0$的时候，左边部分为空，当$i=m$的时候，右半部分为空。&emsp;&emsp;同理,若我们将数组$B$也随机分割: left_B right_B $B[0],B[1],…..,B[j-1]$ $B[j],B[j+1],…..,B[n-1]$ &emsp;&emsp;然后我们将两个数组的左半边和右半边进行合并 left_part right_part $A[0],A[1],…..,A[i-1]$ $A[i],A[i+1],…..,A[m-1]$ $B[0],B[1],…..,B[j-1]$ $B[j],B[j+1],…..,B[n-1]$ 这时，如果满足条件： $len(left_part)=len(right_part)$ $max(left_part)\\leq min(right_part)$ 那么这时候，我们已经将$\\{A,B\\}$分成了长度相等的两个集合，而且有一个集合中的元素总是大于另一个集合中的元素，这样我们就找到了中位数了，即median = \\frac{max(left\\_part)+min(right\\_part）}{2}为了满足上面两个条件，我们只需要满足: $i+j = \\frac {(m+n+1)}{2}$，即$j=\\frac {(m+n+1)}{2}-i$ $A[i-1]\\leq B[j]$ and $B[j-1]\\leq A[i]$ &emsp;&emsp;这里我们假设有$n&gt;m$，由于$0\\leq i \\leq m$，那么$j$就一定为一个非负数。(这里，要是我们将上面的条件改为$i+j=k$，那么就可以查找$k_{th}$的数)所以我们只需要二分查找$i$的位置，即可得到答案。步骤如下所示： 设置左边界$L=0$，右边界$R=m$，从区间$[L,R]$开始查找 让$i=\\frac {L+R}{2}$，$j=\\frac {(m+n+1)}{2}-i$ 现在我们已经满足$len(left_part)=len(right_part)$，只需满足第二个条件即可。 如果满足$A[i-1]\\leq B[j]$ and $B[j-1]\\leq A[i]$，那么我们已经得到了解，退出二分查找即可 如果有$A[i-1] &gt; B[j]$ ，说明我们的$A[i-1]$太大了，所以我们需要减小$i$,由于减小$i$时，$j$会增大，所以能找到我们要的解。而如果我们增大$i$，同时$j$会变小，所以不能得到解，所以此时，我们应该让$R = i-1$,，然后继续查找 如果有$B[j-1] &gt; A[i]$ ，说明我们的$A[i]$太小了，所以我们需要增大$i$,理由同上，所以此时，我们应该让$L = i+1$，然后继续查找 最后当$i$找到的时候，分奇偶两种情况 $max(A[i-1],B[j-1])$，当$m+n$为奇数 $\\frac {max(A[i-1],B[j-1])+min(A[i],B[j])} {2}$，当$m+n$为偶数 &emsp;&emsp;考虑到当$i=0,i=m,j=0,j=n$时，$A[i-1],B[j-1],A[i],B[j]$可能不存在，由于我们只是要判断是否满足条件，所以我们当元素不存在时，就不用进行判断了。也就是说， $(j=0$ || $i=m$ || $B[j-1] \\leq A[i]$ &amp;&amp; $A[i-1] \\leq B[j])$ $(j&gt;0$ &amp;&amp; $i A[i]$)$ $(i&gt;0$ &amp;&amp; $j B[j]$)$ &emsp;&emsp;代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution &#123;public: double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; int m = nums1.size(); int n = nums2.size(); //我们需要保持 n&gt;m if(m&gt;n) &#123; swap(nums1,nums2); swap(m,n); &#125; //二分法查找i int L = 0,R = m; int len = (m+n+1)/2; while(L&lt;=R) &#123; int i = (L + R)/2; int j = len-i; //i太小 if(j&gt;0&amp;&amp;i&lt;R&amp;&amp;nums2[j-1]&gt;nums1[i]) &#123; L = i+1; &#125; //i太大 else if(i&gt;0&amp;&amp;j&lt;n&amp;&amp;nums1[i-1]&gt;nums2[j]) &#123; R = i-1; &#125; //满足两个条件，我们得到了解 else &#123; //首先求左边的最大值 int maxleft = 0; if(i==0) &#123;maxleft = nums2[j-1];&#125; else if(j==0)&#123;maxleft = nums1[i-1];&#125; else&#123;maxleft = max(nums1[i-1],nums2[j-1]);&#125; //判断是否为奇数 if((m+n)%2==1) return maxleft; int minright = 0; if(i == m) minright = nums2[j]; else if(j == n) minright = nums1[i]; else&#123;minright = min(nums1[i],nums2[j]);&#125; return (maxleft+minright)/2.0; &#125; &#125; return 0.0; &#125;&#125;;","path":"2019/01/18/leetcode4/","date":"01-18","excerpt":"","tags":[]},{"title":"LeetCode 第5题 Longest Palindromic Substring 最长回文子串","text":"&emsp;&emsp;这道题实在是很经典了，解法也很多，我只写出了暴力解法-_-||，功力不够啊，把这些解法都学会总结一下，看看是怎么一步一步优化过来的。 题目描述 题目解法 暴力解法 动态规划解法 从中间开始向两边扩展的办法 Manacher’s 算法 题目描述&emsp;&emsp;给定一个子串，找出其中最长的回文子串，注意子串一定要是连续的。 题目解法暴力解法&emsp;&emsp;我的暴力解法思路很简单，从大到小枚举可能的长度$(len,1)$，然后对每个长度枚举起点，然后判断枚举是否为回文串，如果是则终止枚举然后输出。时间复杂度为$O(n^3)$,最低效的做法，并没用上上一个计算出来的回文串结果。1234567891011121314151617181920212223242526272829class Solution &#123;public: string longestPalindrome(string s) &#123; //字符串长度 int len = s.size(); string ans; for(int max_len = len;max_len&gt;=1;max_len--) &#123; for(int start=0;start+max_len&lt;=len;start++) &#123; string substr = s.substr(start,max_len); if(judge(substr)) &#123;return substr;&#125; &#125; &#125; return s; &#125; bool judge(string s) &#123; int len = s.size(); for(int i=0,j=len-1;i&lt;j;i++,j--) if(s[i]!=s[j]) return false; return true; &#125;&#125;; 动态规划解法&emsp;&emsp;暴力解法中，我们可以避免大量的重复判断一个串是否为回文串的不必要计算，动态规划的思想为定义一个数组： dp[i][j],表示从i开始到j结束的子串是否为回文串这样我们可以得到如下的递推式： dp[i][j]=dp[i+1][j-1]\\&\\&s[i]==s[j]动态规划还需要一个初始条件，不难想到为： dp[i][i]=truedp[i][i+1]=s[i]==s[i+1]这样我们就可以通过长度为1的结果，长度为2的结果，从而推出长度为3,4,5…..的结果，动态规划时间复杂度为$O(n^2)$，空间复杂度为$O(n^2)$123456789101112131415161718192021222324252627282930class Solution &#123;public: string longestPalindrome(string s) &#123; int n = s.length(); int longestBegin = 0; int maxLen = 1; bool table[1000][1000] = &#123;false&#125;; for (int i = 0; i &lt; n; i++) &#123; table[i][i] = true; &#125; for (int i = 0; i &lt; n-1; i++) &#123; if (s[i] == s[i+1]) &#123; table[i][i+1] = true; longestBegin = i; maxLen = 2; &#125; &#125; for (int len = 3; len &lt;= n; len++) &#123; for (int i = 0; i &lt; n-len+1; i++) &#123; int j = i+len-1; if (s[i] == s[j] &amp;&amp; table[i+1][j-1]) &#123; table[i][j] = true; longestBegin = i; maxLen = len; &#125; &#125; &#125; return s.substr(longestBegin, maxLen); &#125;&#125;; 从中间开始向两边扩展的办法&emsp;&emsp;我们可以通过进一步优化，使得时间复杂度为$O(n^2)$、空间复杂度为$O(1)$，回文串都是从中间向两边对称扩展的，所以我们可以枚举回文子串中间的位置，一共有$2N-1$种情况，为什么不是$N$呢？注意到，当一个回文串长度为偶数时，其实我们是从两个字符的中间开始枚举的。注意这里处理枚举情况的技巧.123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123;public: string expanding_longeststring(string s,int c1,int c2) &#123; int l = c1,r = c2; int len = s.size(); for(;l&gt;=0&amp;&amp;r&lt;len&amp;&amp;s[l]==s[r];l--,r++); return s.substr(l+1,r-l-1); &#125; string longestPalindrome(string s) &#123; int len = s.size(); if(len==0) return s; string ans; ans = s.substr(0,1); string temp; for(int i = 0;i&lt;len;i++) &#123; temp = expanding_longeststring(s,i,i); if(temp.size()&gt;ans.size()) &#123; ans = temp; &#125; temp = expanding_longeststring(s,i,i+1); if(temp.size()&gt;ans.size()) &#123; ans = temp; &#125; &#125; return ans; &#125;&#125;; Manacher’s 算法&emsp;&emsp;马拉车算法，(⊙o⊙)…，额这个名字挺不错的，这么炫酷的算法不学会简直对不起自己，这个算法的时间复杂度为$O(n)$、空间复杂度为$O(n)$。下面总结一下这个算法。&emsp;&emsp;该算法将原来的字符串进行扩展，将两边和每个字符的中间插入‘#’，例如S = “abaaba”, T = “#a#b#a#a#b#a#”。&emsp;&emsp;然后算法采用了一个数组$P$，来保存当前位置回文串的最大长度，即$P[i]$保存$T$中以第$i$个位置为中心的向左或向右最大能扩展的回文子串长度。也就是$T_{i-P[i]}…..T_{i+P[i]}$为回文子串，例如下面的结果。不难发现，$P[i]$即为原字符串中以$i$为中心的最长回文子串长度。&emsp;&emsp;不难看出，$P$数组具有很强的对称性，那我们能否通过前面计算的$P[i]$从而快速得到后面的$P[i]$呢？答案是可以，算法通过维护两个中间变量$C、R$来快速计算各个$P[i]$，其中$C$代表当前最长回文子串的中心点，$R$表示该子串的右边界，其实有$R=C+P[i]$，下面来分析一下各种情况： &emsp;&emsp;上图中假设我们已经计算出来了$P[0]$到$P[11]$,当前$C=11,R=20,P[C]=9$，假设此时$i=13$，也就是现在我们要计算$P[13]$的值，其中$i=13$关于$C$的对称点为$i’=9$，通过对称性，像上图绿线所示，显然有$P[i]=P[i’]$，所以我们通过这种对称性就可以快速得到$P[i]$的值。那么是否所有的情况都满足这种性质呢？并不是，下面我们来看另外一种情况： &emsp;&emsp;假设我们现在要计算$i=15$的值，那么根据上面的分析，是不是有$P[15]=P[7]=7$呢？显然，并不是。原因是，我们看到由于$P[i’]=7$红色线部分已经超出了我们当前最长回文子串的左边界，同理我们的$P[i]$的对应部分也超出了右边部分，所以超出的部分并不满足对称性，我们不能计算。现在我们知道了，$i$至少能扩展到右边界$R$，也就是$P[i]\\geq 5$，剩下还能扩展多少就需要我们自己判断了，$P[21]!=P[9]$,所以有$P[i]=5$。 &emsp;&emsp;总结一下，分以下情况： $i&lt;R$ $R-i\\leq P[i]$，这种情况下我们的对称区域不会超过右边界，所以我们直接令$P[i]=P[i’]$，其中$i’$为$i$关于$C$的对称点 $R-i &gt;P[i]$，这种情况下我们的对称区域超过了右边界，只知道$P[i]\\geq R-i$，还能扩展多少需要我们自己接下来进行判断 $i\\geq R$ 这种情况下超出了我们的先验知识，我们只能令$P[i]=0$，然后自己接下来一步步判断最大能伸展多少 这一步判断关键代码如下：12i_mirror = 2*C-i;P[i] = (R&gt;i)?min(R-i,P[i_mirror]):0; &emsp;&emsp;现在最后一步就是我们怎么更新$C、R$，很简单，当我们计算$i$位置时，发现将$R$进行了扩展，那我们就让$C=i,R=i+P[i]$。最后我们只需要扫描一遍$P$数组，找出其中索引和最大值就OK了。&emsp;&emsp;扩展$R$最多需要$N$次，而枚举和测试每个中心$C$，也最多需要$N$次。所以算法一共需要$2N$步，时间复杂度为$O(N)$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Solution &#123;public: string preprocessing(string s) &#123; int len = s.size(); string str; if(len==0) str = \"$^\"; else str=\"$\"; for(int i=0;i&lt;len;i++) str+='#'+s.substr(i,1); str+=\"#^\"; return str; &#125; string longestPalindrome(string s) &#123; string str = preprocessing(s); int len = str.size(); int C=0,R =0; //申请保存中间变量的数组 int *P = new int[len+10]; for(int i=1;i&lt;len;i++) &#123; int mirror_i = 2*C - i; //要在边界区域内 P[i] = (R&gt;i)?min(P[mirror_i],(R-i)):0; //下面继续扩充P[i] while(str[i-P[i]-1]==str[i+P[i]+1]) P[i]++; //下面更新C，R if(i+P[i]&gt;R) &#123; C=i; R = i+P[i]; &#125; &#125; int max_length = 0; int idx = 0; //寻找最大值 for(int i=1;i&lt;len;i++) &#123; if(P[i]&gt;max_length) &#123; max_length = P[i]; idx = i; &#125; &#125; delete P; return s.substr((idx-max_length)/2,max_length); &#125;&#125;;","path":"2019/01/18/leetcode5/","date":"01-18","excerpt":"","tags":[]},{"title":"MXNet中目标检测API使用总结：MultiBoxDetection、MultiBoxPrior、MultiBoxTarget","text":"&emsp;&emsp;MXNet在目标检测提供了许多API供用户调用，灵活使用这些函数能大大降低编程难度，其中跟锚框有关的三个函数MultiBoxDetection、MultiBoxPrior、MultiBoxTarget，我也探索了一段时间，下面总结一下这三个函数的用法吧，也方便自己查阅 MultiBoxPrior MultiBoxTarget MultiBoxDetection MultiBoxPrior&emsp;&emsp;该函数是用来在一张给定的特征图中，对于给定的大小比例和宽高比例，生成不同锚框，函数原型为size=5&gt;mxnet.ndarray.contrib.MultiBoxPrior(data=None, sizes=_Null, ratios=_Null, clip=_Null, steps=_Null, offsets=_Null, out=None, name=None, **kwargs)下面总结一下一些常用参数的作用。 参数 作用说明 data 输入的特征图，一般形状为（批量大小，通道数、宽、高） sizes 要生成的锚框的大小比例列表，比如 sizes=[0.2,0.5,0.75] ratios 要生成的锚框的宽高比例列表，比如 ratios=[0.5,1,2] clip boolean 类型变量，设置是否要剪切超出边界的锚框，默认为0 输出：生成的锚框，形状为 （1，锚框总数，4） ，其中最后一维为生成的锚框的四个坐标，均除以了宽和高进行了归一化，锚框总数 = 宽*高*每个像素锚框个数，各个批量之间共享这些锚框。 MultiBoxTarget&emsp;&emsp;该函数是用来对生成锚框进行标记，函数原型为MultiBoxTarget(anchor=None, label=None, cls_pred=None, overlap_threshold=_Null, ignore_label=_Null, negative_mining_ratio=_Null, negative_mining_thresh=_Null, minimum_negative_samples=_Null, variances=_Null, out=None, name=None, **kwargs)下面总结一下一些常用参数的作用。|参数|作用说明||:——:|:——:||anchor |输入的锚框，一般是通过MultiBoxPrior生成，形状为（1，锚框总数，4）||label |训练集的真实标签，一般形状为（批量大小，每张图片最多的真实锚框数，5），第二维中，如果给定图片没有这么多锚框，将会用-1填充空白，最后一维中的元素为 类别标签+四个坐标值（归一化）||cls_pred |对于第一个输入参数，即输入的锚框，预测的类别分数，形状一般为（批量大小，预测的总类别数+1，锚框总数），这个输入的作用是为了负采样（negative_mining），如果不设置负采样，那么这个输入并不影响输出||negative_mining_ratio |设置负采样的比例大小||negative_mining_thresh |设置负采样的阈值||minimum_negative_samples |最少的负采样样本| 其中输出为一个列表，有三个元素，分别为bbox_offset、bbox_mask、cls_labels，注意以上三个输出均为二维矩阵 输出 说明 bbox_offset 每个锚框的标注偏移量，形状为（批量大小，锚框总数*4） bbox_mask 每个锚框的掩码，这些掩码一一对应上面的偏移量，由于我们不关心背景的偏移量，所以负类锚框坐标对应的掩码均为0，正类锚框坐标对应的掩码均为1，形状为（批量大小，锚框总数*4） cls_labels 每个锚框的标注类别，其中0表示为背景，当设置了负采样后，标签为-1表示负采样中被丢弃，计算损失函数时应丢弃该样本，输出形状为 （批量大小，锚框总数） MultiBoxDetection&emsp;&emsp;该函数是用来对预测完毕的锚框进行例如加上偏移量，非极大值抑制等处理，函数原型为MultiBoxDetection(cls_prob=None, loc_pred=None, anchor=None, clip=_Null, threshold=_Null, background_id=_Null, nms_threshold=_Null, force_suppress=_Null, variances=_Null, nms_topk=_Null, out=None, name=None, **kwargs)下面总结一下一些常用参数的作用。 参数 说明 cls_prob 预测的各个锚框的概率，一般要经过$softmax$运算，形状为（批量大小，预测总类别数+1，锚框总数） loc_pred 预测的各个锚框的偏移量，一般形状为 （批量大小，锚框总数*4） anchor 生成的默认锚框，一般形状为（1，锚框总数，4） clip 是否要剪切超出边界的锚框，默认为1 threshold 正类预测的阈值，当对某一个锚框预测的类别置信度大于设置的阈值时，会被当做正类锚框处理，否则视为负类 background_id 背景的类别id，默认为0 nms_threshold 非极大值抑制的阈值 &emsp;&emsp;输出为经过处理之后的锚框，形状为（批量大小，锚框总数，6）其中最后一维组成为，类别标签+置信度+边界框四个坐标（归一化处理），其中类别标签为-1表示为背景或者在NMS中被移除","path":"2019/01/18/mxnetAPIobjectdetection/","date":"01-18","excerpt":"","tags":[{"name":"mxnet","slug":"mxnet","permalink":"https://github.com/JJXiangJiaoJun/tags/mxnet/"},{"name":"物体检测","slug":"物体检测","permalink":"https://github.com/JJXiangJiaoJun/tags/物体检测/"}]},{"title":"LeetCode 第2题 Add Two Numbers","text":"坚持刷LeetCode，有时间每日一题，能学到很多数据结构的内容~ 题目描述&emsp;&emsp;给定两个非空链表，倒序存储着两个数，链表中的每个元素为一个单独数字，假设没有前导零。求这两个数的和，并且以倒序存储的链表形式返回。 Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 分析&emsp;&emsp;这道题目并不难，其实只需要考虑三点，1、考虑两个数相加进位的情况 2、考虑两个链表不等长的情况 3、考虑两个链表最后的数字相加时还有进位的情况。&lt;/br&gt;我的代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; int sum = 0; //创建一个空列表 ListNode* sum_list = new ListNode(0); ListNode* head = sum_list; ListNode* tail = sum_list; //将两个链表中长度一致的部分相加 while(l1 != NULL &amp;&amp; l2 != NULL) &#123; sum += l1-&gt;val+l2-&gt;val; // 计算两者的和 //使用尾插法 tail-&gt;next = new ListNode(sum%10); tail = tail-&gt;next; sum/=10; l1 = l1-&gt;next; //更新l1指针 l2 = l2-&gt;next; //更新l2指针 &#125; ListNode* remain = l1==NULL?l2:l1; while(remain!=NULL) &#123; sum+=remain-&gt;val; tail-&gt;next = new ListNode(sum%10); tail = tail-&gt;next; sum/=10; remain = remain-&gt;next; &#125; while(sum!=0) &#123; ListNode *new_number = new ListNode(sum%10); tail-&gt;next = new ListNode(sum%10); tail = tail-&gt;next; sum/=10; &#125; return head-&gt;next; &#125;&#125;; &emsp;&emsp;答案的代码如下：1234567891011121314151617181920212223242526272829303132/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; ListNode* root = new ListNode(0), *first = root; int carry = 0; while ( l1 || l2 ) &#123; int n1 = l1 ? l1-&gt;val : 0; int n2 = l2 ? l2-&gt;val : 0; int ans = n1 + n2 + carry; carry = ans / 10; root-&gt;next = new ListNode(ans%10); root = root-&gt;next; if ( l1 ) l1 = l1-&gt;next; if ( l2 ) l2 = l2-&gt;next; &#125; if (carry) root-&gt;next = new ListNode(1); return first-&gt;next; &#125;&#125;; 总结&emsp;&emsp;答案的代码有一点很好的思想，就是使用三元运算符，当指针为null时，直接将取出的值赋值为0。以后这种指针题，可以考虑多用几个指针，同时移动，可能会减少编程难度。","path":"2019/01/18/leetcode2/","date":"01-18","excerpt":"","tags":[]},{"title":"物体检测Object Detection学习笔记（MXNet）（三）","text":"&emsp;&emsp;今天总结一下学习的SSD（single shot detector）单发多框物体检测框架，总的来说物体检测处理流程和图像分类的总体流程差不多，只不过多了很多细节。&emsp;&emsp;总结一下，实现一个网络的基本流程： 首先需要定义整体网络中小的功能块，比如说ResNet中的残差小块，GoogLeNet中的inception等，一些可以抽象出来的功能块。 搞清楚每一层的输出形状，以及要进行的处理。比如说SSD中对每一层的输出特征图都要进行预测类别和预测边界框回归量 继承nn.Block定义一个完整的网络，重写实现其中的__init()__函数和forward()函数 获取用于训练的小批量数据 定义损失函数的计算，定义优化器 定义评估方法，如何进行模型评估 训练模型 首先获取用于训练小批量数据 前向运算，获得模型输出 计算损失函数 反向传播 用优化器迭代更新参数 输出模型训练信息，并且重复上述步骤 参考资料：动手深度学习 SSD简介框架说明&emsp;&emsp;SSD是一种多尺度的目标检测模型，SSD主要由一个基础网络快和若干个多尺度特征块串联而成。 基础网络块：一般选择常用的深度卷积神经网络，比如分类层之前截断的VGG，ResNet等，用来提取图像的特征。 多尺度特征块，可以将上一层特征图的高和宽减小，增大每个像素的感受野，越靠近输出层的多尺度特征块输出的特征图越小，基于特征图生成的锚框数量也越小，更加适合检测尺寸较大的目标 SSD具体的检测过程如下图所示： ) SSD中锚框的类别预测层&emsp;&emsp;SSD中对每个特征图中的锚框都要预测类别，如果物体类别数为$q$，那么SSD中预测的类别数应为$q+1$，其中0表示该锚框中只有背景。&emsp;&emsp;为了减少运算量，SSD中采用通道数来表示预测的类别，类别预测层的输出高和宽等于输入的高和宽，而通道数 = 每个像素生成的锚框个数（物体类别数+1），其中输入和输出在特征图在空间坐标上一一对应。比如说第$i$个锚框各个类别的预测分数在第 $(i-1)$\\（物体类别数+1）到 $i$ * （物体类别数+1）通道之间。&emsp;&emsp;从论文中可以看出，预测所用的卷积核大小均为$3*3$，步幅为$1$，填充为$1$，保证输入和输出的大小一致&emsp;&emsp;使用MXNet实现类别预测层代码如下12def cls_predictors(num_anchors,num_classes): return nn.Conv2D(channels=num_anchors*(num_classes+1),kernel_size=3,padding=1) SSD中锚框的偏移量预测层&emsp;&emsp;SSD中对每个特征图中的锚框还需要预测四个偏移量，还是采用通道数来表示预测的偏移量，从论文中可以看出，偏移量预测层输出的通道数应为 $4$每个像素生成的锚框个数。其中第$i$个锚框预测的4个偏移量在 第$(i-1)4$ 到 $i*4$ 个通道中&emsp;&emsp;使用MXNet实现偏移量预测层代码如下12def bbox_predictors(num_anchors): return nn.Conv2D(channels=num_anchors*4,kernel_size=3,padding=1) SSD中高和宽减半模块&emsp;&emsp;由于SSD是多尺度的目标检测框架，所以高和宽减半模块主要是减小输入特征图的宽和高，增大每个像素的感受野，从而使越靠近输出层的特征图每个像素的感受野越大，从论文中可以看到，实现为128通道的$11$卷积，后面接上256通道的$33$卷积，并设置步幅为2以减小输入特征图的尺寸，不过这个网络我们也可以自己设计，只需要一步步增大特征图的感受野即可。下面我采用 128通道的$11$卷积，后面接上256通道的$33$卷积，最后加上$3*3$的MaxPooling，并设置步幅为2。&emsp;&emsp;使用MXNet实现宽和高减半层代码如下123456789def down_sample_blk(num_channels): blk = nn.Sequential() blk.add(nn.BatchNorm(),nn.Activation('relu'), nn.Conv2D(channels=num_channels//2,kernel_size=1,strides=1), nn.BatchNorm(),nn.Activation('relu'), nn.Conv2D(channels=num_channels,kernel_size=3,strides=1,padding=1)) blk.add(nn.MaxPool2D(pool_size=2)) return blk SSD中的基础网络&emsp;&emsp;SSD中的基础网络块用于提取图像特征，论文中采用的是VGG-16，并且将最后两个全连接层，换成了卷积层。通常情况下，基础网络输出的特征图相对来说较大，分辨率较高，所以通常在其上用来检测小物体。&emsp;&emsp;使用MXNet实现VGG代码如下12345678910111213141516171819202122232425262728293031def vgg_blk(num_channels): blk = nn.Sequential() for _ in range(2): blk.add(nn.BatchNorm(), nn.Activation('relu'), nn.Conv2D(channels=num_channels,kernel_size=3,strides=1,padding=1)) blk.add(nn.BatchNorm(),nn.Activation('relu'), nn.Conv2D(channels=num_channels,kernel_size=1,strides=1)) blk.add(nn.MaxPool2D(pool_size=2,strides=2)) return blkdef vgg_16(): vgg_16 = nn.Sequential() conv = (32,64,128,256,512,512) vgg_16.add(nn.BatchNorm(),nn.Activation('relu'), nn.Conv2D(channels=conv[0],kernel_size=7,strides=1,padding=3), nn.BatchNorm(),nn.Activation('relu'), nn.Conv2D(channels=conv[0],kernel_size=3,strides=1,padding=1)) #vgg_16.add(nn.MaxPool2D(pool_size=2,strides=2)) for i in range(3): vgg_16.add(vgg_blk(conv[i+1])) #最后两层换成卷积 vgg_16.add(nn.BatchNorm(),nn.Activation('relu'), nn.Conv2D(channels=conv[4],kernel_size=3,strides=1,padding=1), nn.BatchNorm(),nn.Activation('relu'), nn.Conv2D(channels=conv[5],kernel_size=1)) return vgg_16 连接各个层输入的函数&emsp;&emsp;以上SSD中的所有模块我们都已经构造完成了，由于每一层都会特征图都会输出预测的锚框类别，以及预测的锚框偏移量，我们需要将其拼接起来，以便后续的损失函数计算。我们直接将所有的预测结果拉成一个2D矩阵，其中第一维为批量大小，所有预测结果均在第二维进行拼接。&emsp;&emsp;拼接函数的代码如下所示123456#把通道数换到最后def flatten_pred(pred): return pred.transpose((0,2,3,1)).flatten()def concat_pred(preds): return nd.concat(*[flatten_pred(pred) for pred in preds],dim=1) 定义SSD中的每一层前向运算函数&emsp;&emsp;SSD中每一层的特征图都要有三个输出，分别为①、默认锚框 形状为（1，锚框总数，4） ②、类别预测，形状为（批量大小，每个像素锚框数（总类别数+1），高，宽） ③、偏移量预测，形状为（批量大小，每个像素的锚框数4，高，宽）&emsp;&emsp;所以输入参数中应包括这一个特征图我们想生成锚框的大小比例，以及宽高比例，这是可以由我们自己指定的超参数，我们可以在前面的层生成比较多的小锚框，而在后面的层生成较少的大锚框。&emsp;&emsp;拼接函数的代码如下所示 12345678910def blk_forward(net,X,sizes,ratios,cls_predictor,bbox_predictor): #首先计算这一层的输出 Y = net(X) #生成默认锚框 anchors = contrib.nd.MultiBoxPrior(Y,sizes=sizes,ratios=ratios) #进行预测 cls_preds = cls_predictor(Y) bbox_preds = bbox_predictor(Y) #输出生成的默认锚框，类别预测，偏移量预测 return anchors,cls_preds,bbox_preds 定义SSD完整模型&emsp;&emsp;定义SSD的结构为基础网络+3个宽高减半模块+全局最大池化层，最后将所有层的输出都通过上面定义的concat函数进行拼接，网络最后的输出为 生成的锚框、类别预测、偏移量预测。&emsp;&emsp;SSD结构完整代码如下所示12345678910111213141516171819202122232425262728293031323334353637383940414243def get_blk(i): if i==0: return vgg_16() elif i==4: return nn.GlobalMaxPool2D() else: return down_sample_blk(256)class SSD(nn.Block): def __init__(self,num_classes,**kwargs): super(SSD,self).__init__(**kwargs) self.num_classes = num_classes #需要预测的总类别数 self.sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79], [0.88, 0.961]] self.ratios = [[0.35, 1.5, 0.75]] * 5 #超参数，每一层我们需要生成的锚框宽高比和大小比例 #下面定义每一层的网络 for i in range(5): #这一层的每个像素的锚框个数 num_anchors = len(self.sizes[i])+len(self.ratios[i])-1 #卷积层 setattr(self,'blk_%d' % i,get_blk(i)) #类别预测 setattr(self,'cls_pred_%d' % i,cls_predictor(num_anchors,num_classes)) #锚框偏移量预测 setattr(self,'bbox_pred_%d' % i ,bbox_predictor(num_anchors)) def forward(self,X): #前向运算函数 anchors,cls_preds,bbox_preds= [],[],[] for i in range(5): #进行一次前向运算 X,anchor,cls_pred,bbox_pred = blk_forward(getattr(self,'blk_%d' % i),X,self.sizes[i],self.ratios[i], getattr(self,'cls_pred_%d' % i), getattr(self,'bbox_pred_%d' % i)) #将输出追加到结果中 anchors.append(anchor) cls_preds.append(cls_pred) bbox_preds.append(bbox_pred) #最后将结果输出 return(nd.concat(*anchors,dim=1), concat_pred(cls_preds).reshape((0,-1,self.num_classes+1)), concat_pred(bbox_preds)) 定义损失函数 平滑$L_1$范数损失函数&emsp;&emsp;SSD中将$L_1$范数损失函数替换成为平滑$L_1$范数损失函数。后者在零点附近做了平滑处理，其中可以通过设置超参数$\\sigma$来控制平滑的区域 ) 焦点损失函数 &emsp;&emsp;其中类别预测中，使用了焦点损失：设真实类别$j$的预测概率是$p_j$，交叉熵损失为$- \\log p_j$。那么给定超参数$\\gamma$、$\\alpha$，该损失的定义为： ) 可以通过增大$\\gamma$来有效的减小正类预测概率较大时候的损失。 &emsp;&emsp;我们可以通过继承gluon.loss.Loss类来重写损失函数，只需要重写其中的__init()__，以及hybrid_forward方法即可，MXNet可以帮我们实现自动求导 123456789101112131415161718192021222324252627282930313233class smooth_L1Loss(gloss.Loss): def __init__(self,sigma=0.5,weight=None,batch_axis=0,data_axis=-1,**kwargs): super(smooth_L1Loss,self).__init__(weight,batch_axis,**kwargs) self.axis = data_axis self._sigma = sigma #loss里面有很多函数可以自己来定义loss def hybrid_forward(self,F,pred,label,sample_weight=None): loss = F.smooth_l1((pred-label),scalar=self._sigma) loss = gluon.loss._apply_weighting(F,loss,self._weight,sample_weight) return F.mean(loss,axis = self._batch_axis,exclude =True)#######################################################class focal_SoftMaxCrossEntropy(gloss.Loss): def __init__(self,gamma=2.0,data_axis=-1,batch_axis=0, sparse_label = True,from_logits = False,eps =1e-5,weight=None,**kwargs): super(focal_SoftMaxCrossEntropy,self).__init__(weight,batch_axis,**kwargs) self._gamma = 2.0 self._axis = data_axis self._sparse_label = sparse_label self._from_logits = from_logits self._eps = eps def hybrid_forward(self,F,pred,label,smaple_weight = None): if not self._from_logits: #计算一次softmax pred = F.softmax(pred,axis = self._axis) if self._sparse_label: #这里keep_dim是为了后面与 smaple_weight相乘 pred = nd.pick(pred,label,axis=self._axis,keepdims=True) loss = -((1-pred)**self._gamma)*F.log(pred+self._eps) loss = gluon.loss._apply_weighting(F,loss,self._weight,smaple_weight) return F.mean(loss,axis = self._batch_axis,exclude=True) 定义优化器、以及评价函数&emsp;&emsp;优化器直接使用sgd优化器，评价函数直接使用$L1$损失函数即可 12345678tiny_SSD.initialize(init = init.Xavier(),ctx=ctx)trainer = gluon.Trainer(tiny_SSD.collect_params(),'sgd',&#123;'learning_rate':0.2,'wd':5e-4&#125;)def cls_eval(cls_pred,cls_label): return (cls_pred.argmax(axis=-1)==cls_label).mean().asscalar()def bbox_eval(bbox_pred,bbox_label,bbox_mask): return (bbox_pred*bbox_mask-bbox_label*bbox_mask).abs().mean().asscalar() 获取训练数据，训练模型1234567891011121314151617181920212223242526272829def train(num_epoches): for epoch in range(num_epoches): start = time.time() train_cls_acc = 0 train_bbox_loss = 0 train_iter.reset() for i,batch in enumerate(train_iter): #获取小批量数据 X = batch.data[0].as_in_context(ctx) Y = batch.label[0].as_in_context(ctx) with autograd.record(): #前向运算 anchors,cls_preds,bbox_preds = tiny_SSD(X) #标记锚框获得标签,这里还可以设置负采样 bbox_labels,bbox_masks,cls_labels= contrib.nd.MultiBoxTarget(anchors,Y,cls_preds.transpose((0,2,1))) #计算损失 l_cls = focal_loss(cls_preds,cls_labels) l_bbox = smooth_L1(bbox_preds*bbox_masks,bbox_labels*bbox_masks) l_total = l_cls+l_bbox #反向传播 l_total.backward() #迭代参数 trainer.step(batch_size) train_cls_acc += cls_eval(cls_preds,cls_labels) train_bbox_loss += bbox_eval(bbox_preds,bbox_labels,bbox_masks) #训练完epoch输出结果 print('epoch %2d , train_cls_acc %.2f , bbox mae %.2e , time %.1f sec' % (epoch+1,train_cls_acc/(i+1),train_bbox_loss/(i+1),time.time()-start))","path":"2019/01/18/object-detection-note3/","date":"01-18","excerpt":"","tags":[{"name":"mxnet","slug":"mxnet","permalink":"https://github.com/JJXiangJiaoJun/tags/mxnet/"},{"name":"物体检测","slug":"物体检测","permalink":"https://github.com/JJXiangJiaoJun/tags/物体检测/"}]},{"title":"物体检测Object Detection学习笔记（MXNet）（二）","text":"多尺度物体目标检测 锚框生成过多的问题 尺度 如何在MXNet中生成多尺度的锚框 总结 锚框生成过多的问题&emsp;&emsp;上一节学习到，我们是基于生成的锚框来预测物体类别和偏移量，而且我们对于一张原始图片，对于每个像素都会生成多个锚框。&emsp;&emsp;问题：我们生成了大量的锚框，而在其中有大量的重复区域，造成了运算量过于复杂。那么如何减少我们的计算量呢？&emsp;&emsp;其实很简单，我们只需要对原始图像均匀采样一小部分像素，并以采样的像素为中心生成锚框，这样我们可以通过控制采样的像素个数，而控制整体的运算复杂度。 尺度&emsp;&emsp;尺度这个概念其实在生活中我们常常接触，举一些比较形象的例子，比如说我们用手机拍照时，可以通过缩小放大画面，从而改变尺度。我们在用地图的时候，也可以缩小和放大改变尺度。&emsp;&emsp;计算机在不知道我们感兴趣的物体尺寸情况下，我们就需要考虑多尺度以获得感兴趣物体的最佳尺度。&emsp;&emsp;具体一点到我们的CNN中，我们CNN中一般越靠近输出层的特征图高和宽会越来越小，我们可以看做是，在越靠近输出层的特征图每个像素感受野越大，也就是说每个像素能反映原图片中越大的区域，也就是尺度越大。（比如在靠近输入层地方的特征图，一个像素只能对应原图像中3*3的像素区域，但是在靠近输出层地方的特征图，一个像素有可能就对应30*30的像素区域）&emsp;&emsp;目标检测过程中，较小目标相比于较大目标在图像上出现的位置可能性更多，比如形状为$11,21,22$的目标在形状为$22$的图像区域上出现的位置可能性为$4,2,1$种。所以我们可以在靠近输出层的大尺度特征图中生成大小比例大、数量较少的锚框，来检测尺寸比较大的物体。在前面的一些小尺寸特征图中生成大小比例小、数量较多的锚框，来检测尺寸较小的物体。 如何在MXNet中生成多尺度的锚框&emsp;&emsp;问题来了，我们如何用MXNet中函数生成多尺度的锚框呢？其实很简单，我们直接使用不同大小的特征图作为输入，然后生成锚框，最后还原的时候乘以原始图像的宽和高就OK了。也就是我们可以通过定义一个特征图来对整个原始图像均匀采样。&emsp;&emsp;我们知道生成锚框的函数为contrib.nd.MultiBoxPrior,这个函数的输出为（1，锚框总数，4）其中各个不同的批量共享这些锚框。这个函数输出的四个坐标自动的除以了输入的宽和高，归一化到了0和1之间。这就代表了输出的锚框是相对于整个图像的相对位置，由于我们生成锚框是对特征图每个像素都生成，所以我们基于这个特征图生成的锚框一定是对原始图像均匀采样得到的。123456789101112def show_diff_scale_anchors(axes,fmap_w,fmap_h,sizes,ratios,img_scale): #显示不同尺度的图像 fmap = nd.zeros(shape=(1,3,fmap_h,fmap_w)) #生成锚框 anchors = contrib.nd.MultiBoxPrior(fmap,sizes=sizes,ratios=ratios) #显示多尺度 show_all_bboxes(axes,anchors[0]*img_scale) plt.figure(figsize=(7,7))img = image.imread('../img/catdog.jpg').asnumpy()fig = plt.imshow(img)show_diff_scale_anchors(fig.axes,4,4,[0.15],[1,2,0.5],img_scale) &emsp;&emsp;上面我们在$4*4$的特征图上生成锚框，一共采样了16个像素，每个像素有3个锚框，结果如下： &emsp;&emsp;接下来我们尝试加大尺度，在$2*2$的特征图上生成锚框，此时我们的锚框大小比例应该加大，而个数应该减小，结果如下：1234plt.figure(figsize=(7,7))img = image.imread('../img/catdog.jpg').asnumpy()fig = plt.imshow(img)show_diff_scale_anchors(fig.axes,2,2,[0.3],[0.7,0.5],img_scale) &emsp;&emsp;最后我们尝试最大的尺度，使用$1*1$的特征图，此时我们应该是在中心像素生成锚框，结果如下：1234plt.figure(figsize=(7,7))img = image.imread('../img/catdog.jpg').asnumpy()fig = plt.imshow(img)show_diff_scale_anchors(fig.axes,1,1,[0.75],[0.7,0.5],img_scale) 总结&emsp;&emsp; 不同尺度的特征图在原始输入图像中含有不同的感受野，越靠近输出层的特征图每个像素的感受野越大，越适合用来检测大物体 我们可以使用contrib.nd.MultiBoxPrior函数很方便生成不同尺度的锚框，最后我们还原时乘以输入图像的高和宽就行 SSD就是一种多尺度的目标检测框架","path":"2019/01/18/object-detection-note2/","date":"01-18","excerpt":"","tags":[{"name":"mxnet","slug":"mxnet","permalink":"https://github.com/JJXiangJiaoJun/tags/mxnet/"},{"name":"物体检测","slug":"物体检测","permalink":"https://github.com/JJXiangJiaoJun/tags/物体检测/"}]},{"title":"物体检测Object Detection学习笔记（MXNet）（一）","text":"物体检测比图像分类的难度大得多，过程也复杂了许多。所以希望自己能将自己的学习过程记录下来，总结过程中也许会有不一样的体会。 边界框&emsp;&emsp;目标检测中，通常不止需要我们识别出物体的类别，还需要我们检测出物体的具体位置,所以我们常用边界框来描述物体的具体位置，具体来说通常情况下，我们用物体的左上角 $x,y$ 坐标和右上角 $x,y$ 来标记一个物体的位置，即 $(x_l,y_l,x_r,y_r)$ 来标记物体的位置。&emsp;&emsp;具体实现：其实边界框就是一个矩形框，我们可以使用matplotlib绘制一个矩形框然后显示就行。总结步骤如下 读入图片，例如使用MXNet API mx.image.imread(path).asnumpy()，记住后面需要转换为numpy数据格式，因为matplotlib只支持numpy格式数据，而不支持ndarray。如123456789101112131415161718* 生成边界框坐标，一般用一个数组来保存，如`dog_box,cat_box = [60, 45, 378, 516], [400, 112, 655, 493]`,其坐标按顺序分别为$(x_l,y_l,x_r,y_r)$ * 使用plt绘制出矩形,获得一个矩形框对象，使用`plt.Rectangle(xy, width, height, angle=0.0, **kwargs)`函数进行绘制，所以我们需要将上述的坐标转换为左上角坐标+长宽,具体代码为`dog_bbox=plt.Rectangle((dog_box[0],dog_box[1]),width=(dog_bbox[2]-dog_bbox[0]),height=(dog_bbox[3]-dog_bbox[1]), fill = False,edgecolor = &apos;r&apos;,linewidth=2)` * 最后一步，将我们上述生成的矩形框对象加入到显示的图像中即可，使用`fig.axes.add_patch()`函数实现。 ```pythonimg =image.imread(&apos;../img/catdog.jpg&apos;).asnumpy()dog_bbox,cat_bbox = [60, 45, 378, 516], [400, 112, 655, 493] dog_bbox = plt.Rectangle((dog_bbox[0],dog_bbox[1]), width=dog_bbox[2]-dog_bbox[0],height=dog_bbox[3]-dog_bbox[1], fill = False,edgecolor = &apos;r&apos;,linewidth=2)cat_bbox = plt.Rectangle((cat_bbox[0],cat_bbox[1]), width=cat_bbox[2]-cat_bbox[0],height=cat_bbox[3]-cat_bbox[1], fill = False,edgecolor = &apos;r&apos;,linewidth=2) fig = plt.imshow(img)fig.axes.add_patch(dog_bbox)fig.axes.add_patch(cat_bbox) 锚框定义 &emsp;&emsp;目标检测历史中，RCNN，Fast-RCNN采用的是启发式搜索(selective search)来生成我们感兴趣的目标区域(RoI)，并且在这些区域的基础上提取特征，最后进行分类和回归预测。 &emsp;&emsp;之后的方法如Faster-RCNN、YOLO、SSD等检测框架中，放弃了启发式搜索方法，而是改用 锚框 来生成我们的RoI。 &emsp;&emsp;具体就是，目标检测通常会以每个像素为中心生成多个大小和宽高比不同的边界框。这些边界框就成为锚框，之后的特征提取、分类、回归预测都是基于这些锚框的。 生成方式 &emsp;&emsp;通常我们会选取多组大小比例 $s \\in (0,1)$ 和宽高比 $r &gt; 0$ 的锚框，生成的锚框大小为 $ws \\sqrt r$和 $hs \\sqrt r$。 &emsp;&emsp;假设我们分别设定好了一组大小比例 $s_1,s_2…,s_n$，以及一组宽高比$r_1,r_2,…,r_m$。如果我们以每个像素为中心都使用所有大小的宽高比组合，那么输入图像一共会得到$whnm$ 个锚框。 RCNN中 的确是如此做，直接使用生成的锚框 SSD中为了降低计算复杂度，只使用包含 $r_1$ 或 $s_1$ 的大小和宽高比组合，所以一共有$n+m-1$个锚框。 下面记录一下，MXNet中生成锚框的方式，MXNet中使用contrib.nd.MultiBoxPrior(data=None, sizes=_Null, ratios=_Null, clip=_Null, steps=_Null, offsets=_Null, out=None, name=None, **kwargs)函数来生成锚框。解释一下其中的重要参数： data：输入数据，形状为(批量大小，通道数，宽，高)，函数会基于宽、高为每个像素生成锚框,其中输入数据的前两维 (批量大小,通道数) 对输出没有影响。 sizes：大小比例，即上面说的 $s$ 一般输入为一个列表，表示不同的大小比例 ratios：宽高比例，即上面说的 $w$ 一般输入也为一个列表，表示不同的宽高比例 返回值为 ( 1，生成的锚框总数，4 ) 形状的张量。其中锚框总数为 $hw(n+m-1)$ 并且返回的锚框的坐标值均除以了宽和高。（P.S. 教程中说的是会返回（批量大小，生成的锚框总数，4）形状的锚框，但是我测试了一下发现无论批量大小为多少，其第一维均为1） 绘制锚框的方式那么我们如何绘制所有锚框呢？这里首先总结一下matpoltlib中常用的绘图加标注的函数：|函数|作用||:—-:|:——:||plt.xlabel()|给整幅图像的X轴添加文本标签说明||plt.ylabel()|给整幅图像的Y轴添加文本标签说明||plt.title()|给整幅图形添加文本标题说明||plt.text()|在图像中任意位置添加文本标签说明||plt.annotation()|在图像中任意位置添加带箭头的文本标签说明| &emsp;&emsp;这里我们需要用到的是 plt.text()函数，用于给我们绘制的锚框添加标签，一般我们最后在显示的时候会为每个锚框显示预测类别置信度和类别的标签。下面详细说明一下 plt.text() 中我们需要用到的参数 参数 作用 x 显示位置的x坐标 y 显示位置的y坐标 fontsize 显示文字标签的字体大小 color 显示文字标签的字体颜色 bbox 用于给文字标签生成边界框 其中bbox常用参数如下 bbox参数 作用 boxstyle 边框外形 facecolor 背景颜色 edgecolor 边框颜色 edgewidth 边框线条大小 常用格式如下axes.text(rect.xy[0], rect.xy[1], labels[i],va=&#39;center&#39;, ha=&#39;center&#39;, fontsize=9, color=text_color,bbox=dict(facecolor=color, lw=0))12345678910111213141516171819202122def show_bboxes(axes,bboxes,labels=None,colors=None): def _make_list(obj,defaultvalue=None): if obj is None: obj = defaultvalue elif not isinstance(obj,(list,tuple)): obj = [obj] return obj labels = _make_list(labels) colors = _make_list(colors,['b','g','r','m','c']) #下面开始绘制图像 for i,bbox in enumerate(bboxes): #获取bbox背景颜色 color = colors[i%len(colors)] rect = bbox_to_rect(bbox.asnumpy(),color) axes.add_patch(rect) #添加标签 if labels and len(labels)&gt;i: text_color = 'k' if color=='w' else 'w' axes.text(x=rect.xy[0],y=rect.xy[1],s=labels[i], va='center',ha='center',fontsize=9,color=text_color, bbox=dict(facecolor=color,lw=0)) 注意，当显示通过 contrib.nd.MultiBoxPrior 生成的锚框时，需要将生成的坐标乘以宽和高，这样才是生成坐标的绝对坐标。 交并比&emsp;&emsp; 交并比(Intersection over Union) 作用主要是用来衡量生成的锚框与真实物体框之间的相似度，也就是“距离”。&emsp;&emsp;物体分类中，衡量预测类别和真实类别之间的相似度很简单，直接看预测的概率就行。那么锚框有四个坐标，是一个多标签的区域，我们一般采用交并比来衡量两个区域之间的相似度，具体定义如下： J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.所以我们可以通过交并比，来衡量锚框与真实物框、以及锚框与锚框之间的相似度(用于后面的非极大值抑制)。 如何为每个锚框标注其匹配的真实边界框&emsp;&emsp;图像分类时，我们会将每一张图片都看作一个训练样本，为其标注真实的物体标签（猫、狗、飞机、…..）。这样我们就可以通过预测的标签和真实标签之间的差值，来计算损失函数。&emsp;&emsp;同理在目标检测中，我们将每个锚框都看作一个训练样本，所以我们需要为每个锚框打上标记，为其分配真实标签： 锚框的物体类别（背景、猫、狗、….） 锚框与真实物体框之间的偏移量（如果是背景锚框则不关心，计算偏移量的损失函数的时候并不将其计算在内） &emsp;&emsp;问题：我们如何为一个锚框分配真实边框呢？一张图片中真实物体边框的个数肯定远远小于我们生成的锚框数，也就是说大部分锚框应该都是背景框，给定一个锚框，我们如何为其分配最相似的真实物体框呢？&emsp;&emsp;不妨假设图片中我们生成的锚框为$A_1,A_2,…,A_{n_a}$，真实的物体框为$B_1,B_2,…,B_{n_b}$，显然$n_a&gt;n_b$。定义矩阵 $X \\in \\mathbb R^{n_an_b}$ ，其中第$i$行，第$j$列的元素$x_{ij}$定义为：锚框$A_i$和真实边界框$B_j$的*交并比，那么标记每个锚框的具体步骤如下： 首先找出矩阵所有元素的最大值，也就是找出所有锚框和所有真实物体框中交并比最大的那个，不妨假设该元素的行和列索引为$i_1,j_1$，那么我们为锚框$A_{i_1}$分配真实边界框$B_{j_1}$，之后我们丢弃第$i_11$行所有元素，以及第$j_1$列的所有元素。 重复上一个步骤，我们找到剩余元素中的最大值，假设为行和列索引分别为$i_2,j_2$,那么我们为锚框$A_{i_2}$分配真实物体框$B_{j_2}$,之后我们丢弃第$i_2$行所有元素，以及第$j_2$列的所有元素。 重复上述步骤，直到矩阵的所有$n_b$列都被丢弃为止，那么我们现在已经分配了$n_b$个锚框和真实边界框匹配。只剩下$n_a-n_b$个锚框还未分配。 最后一步，我们遍历剩余的$n_a-n_b$个锚框，也就是我们对剩余的每个锚框$A_i$遍历其所在的行，找出最大的元素，也就是找出与其交并比最大的真实物体框$B_j$，之后如果满足 最大的元素 $&gt;$ 设定的阈值，那么我们为其分配真实边界框$B_j$,否则，将其标记背景锚框（也就是不分配锚框）。 过程如图所示:$x_{23}、x_{71}、x_{54}、x_{92}$分别为每次选出的最大值： &emsp;&emsp;MXNet中已经有函数可以帮我们实现这些功能，这个函数为contrib.nd.MultiBoxTarget()，下面总结一下这个函数各个重要参数说明：参数|作用———-|———-anchor |输入的锚框，一般为MultiBoxPrior生成的锚框，形状为(1，锚框总数，4)，各通道共享这些锚框label |真实的物体框标记，一般形状为（批量大小，最多的真实物体框数，5），第二维为一个训练集中最多的真实物体框数，如果某一张图像没有这么多物体框，将会为其填充-1表示是负样本，这里的第三维为（真实物体框类别+四个边界边界坐标）如[0, 0.1, 0.08, 0.52, 0.92]cls_pred|预测的类别，一般形状为（批量大小，预测的总类别数+1，锚框总数），这个输入的作用是为了负采样（negative_mining），如果不设置负采样，那么这个输入并不影响输出overlap_threshold|前面说的设定的IoU的阈值negative_mining_ratio|负采样的比例，设置了负采样之后，输出只会选择置信度最小的一些负类锚框进行训练，而未选择的负类锚框将标记为-1输入如下图所示 下面说明一下输出，输出为一个list，其中有三个元素，第一个元素为每个锚框标记的四个偏移量，形状为（批量大小，锚框个数*4），其中负类锚框的偏移量标注为0 第二个元素为掩码(mask)，形状为（批量大小，锚框个数*4），与上面生成的每个锚框的四个偏移量一一对应。由于我们不关心对背景的检测，所以负类的偏移量不应该影响目标函数。我们可以通过按元素乘法，从而可以在计算目标函数之前过滤掉负类的偏移量。 最后一个元素为对应锚框标记的类别数，其中0表示背景，然后依次类推，输出的形状为（批量大小，锚框个数）。注意如果我们设置了负采样，那么有一些负类会标记为-1，表示我们丢弃了这些负类样本，在计算损失函数时，应该将其过滤，使用SoftmaxCrossEnrtopy（）函数的sample_weight可以实现过滤。 输出预测边界框&emsp;&emsp;模型预测时，我们会为一张图片生成大量的锚框，并且一一为这些锚框预测类别和偏移量。问题：这些锚框可能会有大量的重复区域，比如所一个物体被框了很多次。解决办法：我们采用非极大值抑制（NMS）来去除重复相似的锚框。&emsp;&emsp;对一个预测边界框$B$，模型会预测它属于各个类别的概率。设其中的最大值为$p$，该概率所对应的类别即为$B$所预测的类别，我们也将$p$称为预测边界框$B$的置信度。&emsp;&emsp;那么具体步骤如下： 在同一张图像上，我们将预测类别非背景的预测边界框置信度，按照从高到低的顺序排序，得到列表。$L$ 在列表$L$中选择置信度最大的预测边界框$B_1$，并且将所有与$B_1$交并比大于设定阈值的非基准边界框，从$L$中移除。 接下来，从$L$中选择置信度第二高的边界框$B_2$作为基准，并且重复上一步的操作 最后直到$L$中所有的边界框都曾经作为基准。此时$L$中剩余的任意一对预测边界框的交并比都小于设定阈值 MXNet中实现这个功能的函数为contrib.ndarray.MultiBoxDetection()下面总结一下其常用的参数参数|作用——-|———-cls_prob|预测各个锚框的概率值，也就是说要经过$\\boldsymbol {softmax}$运算，注意这里的形状为（批量大小，总类别数，锚框总数），和前面有点不一样anchor|预测的所有锚框，形状为（批量大小，锚框总数，4）loc_pred|预测出来的偏移量，形状为（批量大小，锚框总数*4） 输出：输出的形状为（批量大小，锚框总数，6），其中最后一维第一个元素表示锚框的类别，其中-1表示该锚框为背景框或者NMS过程中被删除，最后一维第二各元素为该锚框的置信度，最后一维的最后四个元素为锚框的四个坐标 图像NMS之前如下图所示 进行NMS处理之后，如下图所示 12345678910111213141516171819def show_nms_bbox(axes,nms_bboxes,colors=None): bboxes = [] labels = [] for nms_bbox in nms_bboxes[0].asnumpy(): if(nms_bbox[0]&gt;=0): if nms_bbox[0]==0 : label = 'dog' else: label = 'cat' bboxes.append(nms_bbox[2:]) labels.append(label+str(nms_bbox[1])) bboxes = nd.array(bboxes) print(bboxes) show_bboxes(axes,bboxes*img_scale,labels)plt.figure(figsize=(10,10))fig=plt.imshow(img)show_nms_bbox(fig.axes,outputs) 总结一下关于锚框常用的函数： contrib.nd.MultiBoxPrior用来生成锚框。 contrib.nd.MultiBoxTarget用来为生成的锚框分配真实边界框和标记偏移量。 contrib.nd.MultiBoxDetection用来去掉重复的锚框，NMS。 所有的锚框坐标都除以了宽和高 ，归一化到了（0,1），绘制边界框的时候记得要乘以原来图像的尺寸 参考资料：动手深度学习","path":"2019/01/18/object-detection-note1/","date":"01-18","excerpt":"","tags":[{"name":"mxnet","slug":"mxnet","permalink":"https://github.com/JJXiangJiaoJun/tags/mxnet/"},{"name":"物体检测","slug":"物体检测","permalink":"https://github.com/JJXiangJiaoJun/tags/物体检测/"}]},{"title":"UVa1619 感觉很好","text":"UVa1619 感觉很好 解题报告题目链接&emsp;&emsp;这题使用单调栈，可以在$O(n)$时间内解决，单调栈还是不熟练，总结一下希望能提高吧。 题目分析&emsp;&emsp;题目的意思很简单，实际就是给定一个数组 A，在其中找出一个子序列$A[i]…A[j]$，使其满足$sum(A[i]+A[i+1]+…+A[j])*min(A[i],A[i-1],…,A[j])$为最大值。根据题目的要求，至少要设计出$O(n\\log n)$的算法，当时我有以下几种思路： 枚举起点，然后快速找到终点，不过这题好像没法在$O(1)、O(\\log n)$时间内快速找到终点 枚举子序列的长度，然后用滑动窗口判断，不过好像没法二分枚举，肯定会超时 枚举最小值的可能位置，然后在$O(1)、O(\\log n)$时间内算出，以当前枚举位置为最小值，满足条件的子序列最大值 &emsp;&emsp;用贪心的思想，当最小值一定时，上述式子的长度越长，那么最后算出的值越大，所以我们只要找出，以当前枚举位置为最小值，向左向右最多能延伸多长即可。可以通过预处理出Left[i]、right[i]，表示i位置为最小值时，向左向右最多能延伸的位置。 算法设计&emsp;&emsp;现在的唯一问题就是，如何在$O(n)$时间内得到当前数字向左向右最大的延伸位置？&emsp;&emsp;不难想到，可以用单调栈+滑动窗口来实现，这样在统计的过程中，每个元素至多入队出队一次，时间复杂度为$O(n)$。具体思想就是，假设新加入的元素为 $a$ 那么，将栈中所有 大于等于 $a$ 的元素都出栈。出栈完毕后，栈顶元素位置即为当前元素能延伸到的最大位置，最后将当前元素入栈。&emsp;&emsp;说明如下：因为当栈中的一个元素大于 $a$ 时，它所能延伸到的位置，那么 $a$ 也一定能延伸到，而如果在栈中碰到一个小于 $a$ 的元素时，那么这个栈中元素的位置就是$a$ 能延伸到的最大位置。出栈条件是 $\\geq$ 是因为需要尽量多的延伸，当相等时也要尽量往外延伸。&emsp;&emsp;总结一下单调栈的使用： 首先确定入栈的元素是什么（是数组中的元素，还是数组的下标） 确定单调栈中出栈的条件（大于还是小于，是否有等于） 新加入的元素一定要在最后入栈 注意栈为空的处理 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int maxn = 100000 + 1000;int A[maxn];int n;long long sum[maxn];int stack[maxn];int top;int L[maxn], R[maxn];int kcase = 0;void solve()&#123; memset(sum, 0, sizeof(sum)); //计算前缀和 for (int i = 1; i &lt;= n; i++) sum[i] = sum[i - 1] +(long long) A[i]; //利用单调栈预处理出 left[i] 表示第i个数字最左能延伸到的位置 top = 0; memset(stack, 0, sizeof(stack)); memset(L, 0, sizeof(L)); memset(R, 0, sizeof(R)); for (int i = 1; i &lt;= n; i++) &#123; //栈非空，而且栈顶元素大于当前元素，说明栈顶元素能延伸到的位置，当前元素一定能延伸到 while (top &gt; 0 &amp;&amp; A[stack[top - 1]] &gt;= A[i]) top--; L[i] = top == 0 ? 1 : (stack[top - 1] + 1); //当前元素入栈 stack[top++] = i; &#125; top = 0; memset(stack, 0, sizeof(stack)); for (int i = n; i &gt;= 1; i--) &#123; while (top &gt; 0 &amp;&amp; A[stack[top - 1]] &gt;= A[i]) top--; R[i] = top == 0 ? n : (stack[top - 1] - 1); stack[top++] = i; &#125; int left=1, right=1; long long max_val = 0; for (int i = 1; i &lt;= n; i++) &#123; long long temp_val = (sum[R[i]] - sum[L[i] - 1])*(long long )A[i]; if (temp_val &gt; max_val ||( temp_val == max_val &amp;&amp; (right - left) &gt; (R[i] - L[i]))) &#123; max_val = temp_val; left = L[i]; right = R[i]; &#125; &#125; printf(\"%lld\\n%d %d\\n\", max_val, left, right);&#125;int main()&#123; freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\in.txt\", \"r\", stdin); freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\out.txt\", \"w\", stdout); while (scanf(\"%d\", &amp;n) == 1 &amp;&amp; n) &#123; if (kcase++) printf(\"\\n\"); for (int i = 1; i &lt;= n; i++) scanf(\"%d\", &amp;A[i]); solve(); &#125; return 0;&#125;","path":"2019/01/18/UVa1619/","date":"01-18","excerpt":"","tags":[]},{"title":"UVa11536 Smallest Sub-Array","text":"UVa11536 Smallest Sub-Array解题报告题目链接&emsp;&emsp;又是一个优化枚举次数的题目，这次一定要记录下来，学会这种解题思路。 题目分析&emsp;&emsp;题目的意思是，需要在一个数组中，找到包含$[1,K]$中所有整数的最短长度的子数组。&emsp;&emsp;当时我的思路是通过枚举起点，然后通过二分法来找到终点。然后超时了。。。。然后发现是我的枚举对象搞错了，以后解题要首先明确枚举对象是什么。&emsp;&emsp;根据网上的解题报告，可以选满足条件的最短长度作为枚举对象，然后二分查找，由于满足条件的长度$1\\leq len \\leq N$ ，每次枚举后通过滑动窗口来判断是否满足条件，时间复杂度为$O(N)$，所以总共的时间复杂度为$O(N\\log N)$ 算法设计&emsp;&emsp;取枚举区间为左闭右开区间，初始L=1、R=N+1。如果当前枚举的值满足条件说明还可以继续往小了分，否则就应该往大了分。&emsp;&emsp;判断是否满足条件用滑动窗口法，假设当前枚举长度为$len$，那么我们通过扫描一遍数组，每次只需要判断右边滑进来的元素、和左边滑出去的元素，这样就可以在$O(N)$时间内得到结果。 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;iostream&gt;using namespace std;#define INF 0x3f3f3f3fconst int maxn = 1000000 + 10000;const int maxk = 1000+200;int seq[maxn];int cnt[maxk];//前缀和int N, M, K;//尝试用长度为len的滑动窗口bool judge(int len)&#123; //用来保存在滑动窗口中出现数字的总数 int tot = 0; //cnt[i]记录出现数字i的次数 memset(cnt, 0, sizeof(cnt)); //滑动窗口 for (int i = 1; i &lt;= N; i++) &#123; if (tot == K) return true; if (i == N + 1) return false; //滑动窗口新加入的元素 if (seq[i] &lt;= K&amp;&amp;cnt[seq[i]]++ == 0) tot++; //滑动窗口滑出去的元素 if (i &gt; len&amp;&amp;seq[i - len] &lt;= K&amp;&amp;--cnt[seq[i - len]] == 0) tot--; &#125; if (tot == K) return true; return false;&#125;//二分枚举答案的长度int solve()&#123; int L = 1; int R = N + 1; while (L &lt; R) &#123; int mid = (L + R) / 2; if (judge(mid)) R = mid; else L = mid + 1; //printf(\"mid = %d\\n\", mid); &#125; if (L == N + 1) return -1; return L;&#125;void init()&#123; seq[1] = 1; seq[2] = 2; seq[3] = 3; for (int i = 4; i &lt;= N; i++) &#123; seq[i] = (seq[i - 1] + seq[i - 2] + seq[i - 3]) % M + 1; &#125;&#125;int main()&#123; int T; int kcase = 1; freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\in.txt\", \"r\", stdin); freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\out.txt\", \"w\", stdout); scanf(\"%d\", &amp;T); while (T--) &#123; scanf(\"%d%d%d\", &amp;N, &amp;M, &amp;K); //printf(\"N = %d M = %d K=%d\\n\", N, M, K); init(); int min_len = solve(); if (min_len &gt; 0) printf(\"Case %d: %d\\n\", kcase++, min_len); else printf(\"Case %d: sequence nai\\n\", kcase++); &#125; return 0;&#125; 以后记得选取正确的枚举对象~","path":"2019/01/18/UVa11536/","date":"01-18","excerpt":"","tags":[{"name":"二分法","slug":"二分法","permalink":"https://github.com/JJXiangJiaoJun/tags/二分法/"}]},{"title":"UVa1618 弱键","text":"UVa1618解题报告题目链接这题目又是用到二分法，感觉自己对二分用的还是不很灵活，总结一下思路，希望自己能有点突破吧~ 思路分析&emsp;&emsp;根据题目意思，给定一个数组判断其中是否含有四个满足如下条件的元素： $N_p,N_q,N_r,N_s$ 满足条件$1&lt;=p","path":"2019/01/18/UVa1618/","date":"01-18","excerpt":"","tags":[{"name":"暴力枚举","slug":"暴力枚举","permalink":"https://github.com/JJXiangJiaoJun/tags/暴力枚举/"},{"name":"二分法","slug":"二分法","permalink":"https://github.com/JJXiangJiaoJun/tags/二分法/"}]},{"title":"论文笔记 Deep Learning for Generic Object Detection A Survey （一）","text":"&emsp;&emsp;本文总结了近十多年来物体检测（object detection）方面的进展，对每个里程碑式的成果都做了介绍，自己在读过程中也了解了很多，希望能把自己的体会和学习过程记录下来吧。 总体介绍 问题描述 难点&amp;挑战 过去20年来的发展 物体检测框架 Region Based（Two Stage Framework） RCNN SPPNet Fast-RCNN Faster-RCNN 总体介绍问题描述&emsp;&emsp; 物体检测目标：给定一张任意的图像，检测其中是否含有给定类别的物体，如果含有，则返回其位置和大小。&emsp;&emsp;发展历程: Image level object classification -&gt; single object localization -&gt; generic object detection -&gt; pixel-wise object segmantation 物体分类(object classification) 给定一幅图像，输出其中含有物体的标签，并不需要定位如图中(a)所示 物体检测(object detection) 给定一幅图像，不仅需要检测出其中含有的物体，并且需要对其进行定位，用边界框将所有物体标记出来，如图中(b)所示 语义分割(Semantic Segmentation) 将一幅图像中的每个像素都分配一个物体类别标签，如图中（c）所示 实例分割(Instance Segmentation 不同于语义分割，实例分割中，对于每个相同类别的实例也会进行分割，如图中（d）所示 难点&amp;挑战 准确率 the vast range of intraclass variation 大量的类内变化同样的子类有着不同颜色、材质、形状等，同样的物体有着不同的姿态，不同的拍摄时间、地点、天气状况、背景等等。 huge number of object categories 大量的物体类别 一共$10^4-10^5$种、有些类别之间只有细微的差别 高效性移动设备、可穿戴设备只有有限的存储空间和计算能力，需要降低算法总体的运算复杂度。 过去20年来的发展 &emsp;&emsp;在1990年以前，典型的物体检测方法是基于 geometric representations，之后物体检测的方法像统计分类的方向发展(神经网络、SVM、Adaboost等）。&emsp;&emsp;在20世纪晚期和21世纪早期，物体检测领域取得了突破性的进展，正如图中所示，SIFT 和 DCNN 开启了新的纪元。Handcrafted local invariant feature（人工提取图像特征） 变得流行起来，比如Scale Invariant Feature Transform(SIFT)、Histogram of Gradients(HOG)、Local Binary Patterns(LBP)等都是特征提取的方法。&emsp;&emsp;2012年当深度神经网络(DCNN)在图像分类上取得了突破性进展时，这个巨大的成功也被用到了物体检测上。Girshick提出了里程碑式的物体检测模型Region based CNN(RCNN)，在此之后物体检测领域飞速发展、并且提出了许多基于深度学习的方法…… 物体检测框架 &emsp;&emsp;物体检测框架发展如上图所示。上面的框架可以分为下面两个类别： Two stage detection framework，包括生成 region proposal 的预处理步骤，以及之后的物体分类以及bounding box 回归。 One stage detection framework ，不需要region proposal的生成，直接通过一遍前向运算就能完成物体检测的任务。 Region Based（Two Stage Framework）RCNN&emsp;&emsp;Region based CNN 是 Girshick 受到 CNN 在图像分类上的巨大成功，以及selective search(用于生成 region proposal)等人工特征提取方法的成功鼓舞而提出来的，这是一个里程碑式的物体检测框架，后续的框架或多或少都参考了RCNN，训练和测试具体步骤如下图所示： 训练过程 首先采用selective search 方法生成2000个region proposal（目标区域）。 将生成的目标区域通过warp操作（缩放）到统一尺寸（为了以后的全连接层），作为提前训练好的CNN模型的输入数据(finetune)。 用CNN提取出 feature map，并训练一系列线性SVM分类器，用这些分类器取代最后一层的softmax层。 对每个类别都会用CNN提取的特征训练一个bounding box regression 模型，用于边界框的回归预测。 缺点 训练过程是muti-stage，每一个部分都必须单独进行训练、复杂、低效 训练SVM和回归器需要大量的时间和空间 测试速度非常慢，每个候选窗口都要输入到CNN中进行特征提取 SPPNet&emsp;&emsp;由于CNN的特征提取是RCNN的一个主要瓶颈，SPPNet中提出了Spatial Pyramid Pooling Layer（空间金字塔池化层 ）。由于存在FC（全连接）层，所以RCNN输入的图像需要固定尺寸，并且需要进行warp操作，SPPNet中在最后一层卷积层后加入了SPP层，经过SPP层后可以获得固定长度的特征向量。&emsp;&emsp;通过这种方式，可以让网络输入任意尺寸的图片，并且最终会输出固定尺寸的特征。步骤 首先采用selective search 方法生成2000个region proposal（目标区域）。和RCNN一样 _特征提取阶段_。这一步是和RCNN最大的不同，先把整张图片输入CNN进行特征提取，得到feature map，然后在feature map 中找到各个候选框区域，再对各个候选框进行金字塔池化，从而最后可以提取出固定长度的特征向量。由于SPPNet中只需要对整张图片用CNN进行一次特征提取，于是相比于RCNN速度大大提高。 最后一步也和RCNN一样，采用SVM算法进行特征向量的分类识别，以及boundingbox 的回归。Fast-RCNN&emsp;&emsp;改进了RCNN以及SPPNet的缺点，实现了end-to-end的训练。并提出了RoI pooling层，用于对特征图的池化从而得到固定大小的特征向量。创新 定义了一个多任务的损失函数，同时包括softmax、BBRs等训练目标。 采用了共享卷积计算的思想 在最后一层卷积层上对每个RoI求映射关系，并用一个RoI pooling Layer 来统一到相同大小。 具体步骤和RCNN相似。 Faster-RCNN创新点 用RPN(Region Proposal Network)取代selective search，大大提升了整体运行速度。 通过RPN来生成目标候选框，之后在最后一层卷积层得到的feature map上进行RoI pooling从而得到固定大小的特征图，最后进行分类和回归。 未完待续……………………","path":"2019/01/18/paper-note/","date":"01-18","excerpt":"","tags":[{"name":"论文","slug":"论文","permalink":"https://github.com/JJXiangJiaoJun/tags/论文/"}]},{"title":"JSP实现网页的自动登录(session+cookie)实现","text":"1、session的作用&emsp;&emsp;用户用浏览器访问web服务器时，会为每个用户创建一个session(会话)，session在用户关闭浏览器之前都是有效的，所以我们可以在session中保存用户的一些信息，以供使用。这里我们用session保存用户的账号名称，以表示用户已经登录，可以直接访问后台页面。 2、cookie的作用&emsp;&emsp;cookie是客户端保存的一些少量数据，每次用户通过浏览器访问web服务器时，cookie可以通过request一起传送至服务器端，这里我们使用cookie保存用户的账号密码，以便实现自动登录功能。 说明:实现过程分为两个页面 login.jsp 是前台登录界面分别有账号、密码输入区，以及一个是否自动登录的复选框，一个选择保存时长的下拉列表框，如下图所示: mypage.jsp是登录后的显示界面，有一个注销按钮，以便用户登出。 3、流程 用户访问login.jsp进行第一次登录，对应的servlet将提交的用户名与密码和数据库中的记录进行匹配，如果： 匹配失败（要考虑用户名和密码为空的情况），则返回login.jsp进行再次登录，并且报告错误信息(记住提交URL参数中的中文需要用URLEncoder进行编码，不然会乱码) 匹配成功 （cookie是保存在客户端的，程序中进行配置之后一定需要用response.addCookies将cookie添加至response从而发回客户端浏览器） 如果选择了未自动登录，此时从request中取出原来的cookies,将有效时间设为0，表示不再进行自动登录。 如果选择了自动登录，那么就要创建两个新的cookie，一个保存username，一个保存passwd（安全的做法是用MD5等加密算法加密后保存），并根据选择的有效时间设置cookie的有效时间 之后用reque.getsession.setAttribute()设置username属性表示用户已经登录，最后将页面重定向至mypage.jsp 代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113 protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // TODO Auto-generated method stub String command= request.getParameter(\"command\"); //这里是用户登录的界面，需要处理逻辑关系，考虑 if(command.equals(\"login\")) &#123; //1、没有输入用户名 2、用户名错误 3、密码错误 String username = request.getParameter(\"username\"); String passwd = request.getParameter(\"passwd\"); //如果未输入用户名 if(username.equals(\"\")||passwd.equals(\"\")) &#123; System.out.println(\"未输入用户名\"); String error_msg = \"请输入用户名和密码\"; String error =URLEncoder.encode(error_msg, \"utf-8\"); response.setCharacterEncoding(\"utf-8\"); response.sendRedirect(\"/MyDBtest/login.jsp?error=\"+error); &#125; //对用户名进行查询 else &#123; String real_passwd = loginChecker.getPasswd(username); //没有此用户 if(real_passwd.equals(\"\")) &#123; System.out.println(\"没有此用户\"); String error_msg = \"没有此用户，请重新输入用户名\"; String error =URLEncoder.encode(error_msg, \"utf-8\"); response.setCharacterEncoding(\"utf-8\"); response.sendRedirect(\"/MyDBtest/login.jsp?error=\"+error); &#125; //查询成功 检查用户是否设置自动登录，如果是则发送cookies else if(real_passwd.equals(passwd)) &#123; //设置session username属性表示用户已经登录 request.getSession().setAttribute(\"username\", username); //检查用户是否选择自动登录 //如果选择了自动登录，则需要保存cookies if(\"on\".equals(request.getParameter(\"autologin\"))) &#123; int saveTime = Integer.parseInt(request.getParameter(\"maxage\")); saveTime =24*60*60*saveTime; Cookie user_cookie = new Cookie(\"username\", username); Cookie passwd_cookie = new Cookie(\"passwd\", passwd); user_cookie.setMaxAge(saveTime); passwd_cookie.setMaxAge(saveTime); response.addCookie(user_cookie); response.addCookie(passwd_cookie); &#125; //如果没有选择自动登录 else &#123; Cookie[] cookies = request.getCookies(); for(Cookie cookie:cookies) &#123; System.out.println(\"删除 cookie\"); if(cookie.getName().equals(\"username\")) &#123; cookie.setMaxAge(0); response.addCookie(cookie); //添加生成的新的cookie &#125; else if(cookie.getName().equals(\"passwd\")) &#123; cookie.setMaxAge(0); response.addCookie(cookie); //添加生成的新的cookie &#125; &#125; &#125; //重定向到后台界面 response.sendRedirect(\"/MyDBtest/mypage.jsp\"); &#125; //密码错误 else &#123; System.out.println(\"密码错误\"); String error_msg = \"密码错误，请重新输入密码\"; String error =URLEncoder.encode(error_msg, \"utf-8\"); response.setCharacterEncoding(\"utf-8\"); response.sendRedirect(\"/MyDBtest/login.jsp?error=\"+error); &#125; &#125; &#125; else if(command.equals(\"deleteCookies\")) &#123; //用户主动注销，那么不用再保存Cookies Cookie[] cookies = request.getCookies(); for(Cookie cookie:cookies) &#123; if(cookie.getName().equals(\"username\")) &#123; cookie.setMaxAge(0); response.addCookie(cookie); //设置失效 &#125; else if(cookie.getName().equals(\"passwd\")) &#123; cookie.setMaxAge(0); response.addCookie(cookie); //设置失效 &#125; &#125; //重定向到登录界面 request.getSession().removeAttribute(\"username\"); response.sendRedirect(\"/MyDBtest/login.jsp\"); &#125; &#125;&#125; 对于访问mypage.jsp，步骤如下： 首先检查session是否有username属性，如果存在那么可以直接访问mypage.jsp，若不存在则进行下一步判断 取出request中的cookie(考虑为空的情况)，取出用户名和密码，与数据库中记录进行匹配，若匹配成功，则设置session的username属性，表示已经登录成功，然后跳转至mypage.jsp 若与数据库匹配失败或者cookie为空，那么可将页面重定向至错误页面，提示用户进行登录后才能访问。 不难想到，应该为 mypage.jsp设置一个过滤器，每次访问该页面，先由过滤器完成上述流程，过滤器配置如下：123456789&lt;filter&gt; &lt;filter-name&gt;LoginFilter&lt;/filter-name&gt; &lt;filter-class&gt;Filter.LoginFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;LoginFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/mypage.jsp&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 过滤器代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; // TODO Auto-generated method stub // place your code here //访问 mypage 页面前需要经过此过滤器 //System.out.println(\"进入到后台登录过滤器\"); HttpServletRequest req = (HttpServletRequest)request; HttpServletResponse res = (HttpServletResponse)response; //这里说明用户未登录，那么获取cookies并验证是否能登录 if(req.getSession().getAttribute(\"username\")==null ) &#123; System.out.println(\"进入到后台登录过滤器\"); Cookie[] cookies = req.getCookies(); String username=null; String passwd=null; if(cookies!=null) &#123; for(Cookie cookie:cookies) &#123; if(cookie.getName().equals(\"username\")) &#123; username = cookie.getValue(); &#125; else if(cookie.getName().equals(\"passwd\")) &#123; passwd =cookie.getValue(); &#125; &#125; &#125; if(username!=null&amp;&amp;passwd!=null) &#123; //匹配数据中的用户名以及密码 //匹配成功则设置登录，否则退出登录 if(loginChecker.getPasswd(username).equals(passwd)) req.getSession().setAttribute(\"username\", username); &#125; &#125; // pass the request along the filter chain chain.doFilter(req, res); &#125; mypage.jsp页面还有一个注销按钮，用户注销后,servlet应该主动删除用户的cookie信息。","path":"2019/01/18/jsp/","date":"01-18","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://github.com/JJXiangJiaoJun/tags/java/"},{"name":"jsp","slug":"jsp","permalink":"https://github.com/JJXiangJiaoJun/tags/jsp/"},{"name":"servlet","slug":"servlet","permalink":"https://github.com/JJXiangJiaoJun/tags/servlet/"}]},{"title":"本地仓库git push命令提交到github远程仓库时，出现 更新被拒绝，因为远程仓库包含您本地尚不存在的提交。解决方案","text":"问题说明&emsp;&emsp;出现这种问题是由于本地仓库与远程仓库存在冲突导致的，一般是由于先有的本地仓库，然后创建远程仓库并用 git remote add 命令进行关联。此时，远程仓库存在 README.md和.gitignore文件，而本地仓库不存在，此时使用git push提交命令则会报错，错误如下所示：123456! [rejected] master -&gt; master (fetch first)error: 无法推送一些引用到 &apos;git@github.com:JJXiangJiaoJun/My_JSP.git&apos;提示：更新被拒绝，因为远程仓库包含您本地尚不存在的提交。这通常是因为另外提示：一个仓库已向该引用进行了推送。再次推送前，您可能需要先整合远程变更提示：（如 &apos;git pull ...&apos;）。提示：详见 &apos;git push --help&apos; 中的 &apos;Note about fast-forwards&apos; 小节。 解决方案&emsp;&emsp;我的解决方案是使用以下命令：&emsp;&emsp;&emsp;&emsp;git pull origin master --allow-unrelated-histories&emsp;&emsp;这样对本地仓库和远程仓库进行合并冲突后，就可以正常愉快的使用 git push命令啦~","path":"2019/01/18/gitpush_deny/","date":"01-18","excerpt":"","tags":[]},{"title":"Ubuntu系统生成SSH秘钥并添加到github远程仓库(解决git push被拒绝的问题)","text":"本文主要解决，在一台新电脑本地仓库关联github远程仓库后，用 git push 命令出现Permission denied (publickey)的问题出现这种问题是由于你使用ssh的git pull，而github没有添加本台电脑的ssh秘钥,所以访问会被拒绝，解决步骤分为两步： 第一步ls ~/.ssh 查看目录 ~/.ssh下是否存在文件id_rsa.pub 如果存在就不同再用ssh命令生成秘钥了，直接跳转至第二步，如果没有则输入以下命令: ssh-keygen -t rsa -C &quot;youmailaddress@xx.com&quot; P.S. 后面为什么要加上自己的邮箱我也不太清楚-_-||，网上说是尽量不要让一些无关的因素干扰。&emsp;&emsp;之后会出现如下所示的画面：Generating public/private rsa key pair.Enter file in which to save the key (/home/zqhzju/.ssh/id_rsa):路径默认就OK，直接回车Enter passphrase (empty for no passphrase):输入自己的密码&emsp;&emsp;完成后在 ~/.ssh目录下会生成 id_rsa.pub文件，打开并复制全部内容 第二步&emsp;&emsp;接下来我们要把本地的id_rsa.pub文件中的内容拷贝到你的github账户中，打开你的github主页，点击右上角图标，并选择settings： &emsp;&emsp;之后点击 SSH and GPG keys，然后点击右上角New SSH Key，会出现添加界面，其中title中随便取名字，key中为id_rsa.pub中的内容 完成之后点击 Add SSH key&emsp;&emsp;然后就能愉快的使用git push、git pull 命令对远程仓库进行操作啦~","path":"2019/01/18/UbuntSSH/","date":"01-18","excerpt":"","tags":[{"name":"git命令","slug":"git命令","permalink":"https://github.com/JJXiangJiaoJun/tags/git命令/"}]},{"title":"2018百度之星程序设计大赛（资格赛） 子串查询 HDU6345 解题思路","text":"题目链接 HDU 6345 子串查询 1、题目分析 2、细节思路 3、算法设计 4、程序代码 1、题目分析&emsp;&emsp;本题只要看懂了题意其实还是不难的，题目意思是要求出给定区间中最小子串的个数，所以1、找到最小子串 2、求出最小子串的个数&emsp;&emsp;根据题意，其实最小子串就是给定区间中字典序最小的单个字母，明白了这点，那么本题的就是求解，给定区间中字典序最小的单个字母出现的次数 2、细节思路&emsp;&emsp;根据题目，数据的数量级为$10^5$,暴力查询求解肯定会超时，不难想到，其实这个字典序最小字母次数是满足区间加法的，假设给定两个相连区间$[a,b]$和$[b,c]$，两个区间中最小字母分别为$x1,x2$,出现次数分别为$t1,t2$,那么两个区间合并后的区间为$[a,c]$，合并之后区间的最小字母和出现次数分别记为$x3,t3$。不难得到，有以下情况: 条件 结论 $x1=x2$ $x3=x1=x1$&emsp;$t3=t1+t2$ $x1&gt;x2$ $x3=x2$&emsp;$ t3=t2$ $x1&lt;x2$ $x3=x1$&emsp;$t3=t1$ 满足区间加法的问题都能用线段树来解决，所以此题的关键在于线段树的编写。 3、算法设计主要是按照上面的表格重写了线段树的 $+,=,&gt;,&lt;$等运算符，如下所示 123456789101112131415161718192021222324252627282930313233343536373839//注意 ！！！重载运算符 + 不要改变本身对象struct Sum &#123; int number; char ch; Sum() &#123; ch = 'Z'+1; number = 0; &#125; Sum operator +(const Sum &amp;rhs) &#123; Sum temp; temp.number = number; temp.ch = ch; if(temp.ch==rhs.ch) temp.number= number+rhs.number; else if (temp.ch &gt; rhs.ch) &#123; temp.number = rhs.number; temp.ch = rhs.ch; &#125; return temp; &#125; Sum&amp; operator =(const Sum &amp;rhs) &#123; ch = rhs.ch; number = rhs.number; return *this; &#125; bool operator &gt;(const Sum &amp;rhs) &#123; return ch &gt; rhs.ch; &#125; bool operator &lt;(const Sum &amp;rhs) &#123; return ch &lt; rhs.ch; &#125; bool operator == (const Sum &amp;rhs) &#123; return ch == rhs.ch; &#125;&#125;; 这里每一个sum有两个属性,ch表示该区间的最小字母,number代表最小字母出现次数。 4、程序代码其他部分的代码和普通的线段树没什么区别，全部代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;string&gt;#include &lt;algorithm&gt;#define mem(a,b) memset(a,b,sizeof(a))#define NUM_OF_CHAR 26using namespace std;const int maxn = 100000 + 1000;char A[maxn];int T;int length,q;//注意 ！！！重载运算符 + 不要改变本身对象struct Sum &#123; int number; char ch; Sum() &#123; ch = 'Z'+1; number = 0; &#125; Sum operator +(const Sum &amp;rhs) &#123; Sum temp; temp.number = number; temp.ch = ch; if(temp.ch==rhs.ch) temp.number= number+rhs.number; else if (temp.ch &gt; rhs.ch) &#123; temp.number = rhs.number; temp.ch = rhs.ch; &#125; return temp; &#125; Sum&amp; operator =(const Sum &amp;rhs) &#123; ch = rhs.ch; number = rhs.number; return *this; &#125; bool operator &gt;(const Sum &amp;rhs) &#123; return ch &gt; rhs.ch; &#125; bool operator &lt;(const Sum &amp;rhs) &#123; return ch &lt; rhs.ch; &#125; bool operator == (const Sum &amp;rhs) &#123; return ch == rhs.ch; &#125;&#125;;Sum sum[maxn &lt;&lt; 2];//更新节点的函数inline void PushUp(int rt)&#123; int left = rt &lt;&lt; 1; int right = rt &lt;&lt; 1 | 1; //每个字母的个数分别为 左子树和右子树每个字母的个数之和 sum[rt] = sum[left] + sum[right]; //printf(\"sum[%d] = sum[%d] + sum[%d]\\n\", rt, left, right);&#125;//建立线段树void Build(int l, int r, int rt)&#123; //printf(\"Build(l = %d ,r = %d, rt = %d)\\n\", l, r, rt); //如果到了叶子节点那么保存该节点的值 if (l == r) &#123; //该字母出现一次 sum[rt].number = 1; sum[rt].ch = A[l]; //printf(\"到达叶子节点rt = %d sum[%d].number = %d sum[%d].ch = %c\\n\", rt, rt, sum[rt].number, rt, sum[rt].ch); return; &#125; //计算中点 int m = (l + r) &gt;&gt; 1; //递归建立左右子树 Build(l, m, rt &lt;&lt; 1); Build(m + 1, r, rt &lt;&lt; 1 | 1); //左右子树建立完毕之后 建立本节点 PushUp(rt);&#125;//查询函数，查找在(L,R)中的每个字母的和Sum Query(int L, int R, int l, int r, int st)&#123; //printf(\"L = %d , R= %d, l= %d ,r = %d , st =%d \\n\", L, R, l, r, st); //如果范围在 (L,R）之中，那么直接返回值 if (L &lt;= l&amp;&amp;R &gt;= r) &#123; //printf(\"return sum[%d]\\n\", st); //for (int i = 0; i &lt; 5; i++) // printf(\"sum[%d].ch[%d] = %d \", st, i, sum[st].ch[i]); //printf(\"\\n\"); return sum[st]; &#125; Sum ans; //mem(&amp;ans, 0); //printf(\" 递归计算\\n\"); //for (int i = 0; i &lt; 5; i++) // printf(\"初始化ans.ch[%d] = %d \", i, ans.ch[i]); //printf(\"\\n\"); int m = (l + r) &gt;&gt; 1; //查找与子区间是否存在并集，如果不存在则不用查找 if (m &gt;= L) ans = ans + Query(L, R, l, m, st &lt;&lt; 1); if (m &lt; R) ans = ans + Query(L, R, m + 1, r, st &lt;&lt; 1 | 1); //for (int i = 0; i &lt; 5; i++) // printf(\"ans.ch[%d] = %d \",i, ans.ch[i]); //printf(\"\\n\"); //统计完毕返回 return ans;&#125;void solve()&#123; int left_bound; int right_bound; Sum my_ans; while (q--) &#123; scanf(\"%d%d\", &amp;left_bound, &amp;right_bound); //printf(\"left_bound = %d,right_bound = %d \\n\", left_bound, right_bound); my_ans = Query(left_bound, right_bound, 1, length, 1); //for (int i = 0; i &lt; 5; i++) // printf(\"my_ans.ch[%d] = %d \", i, my_ans.ch[i]); //printf(\"\\n\"); printf(\"%d\\n\", my_ans.number); &#125;&#125;int main()&#123; int kcase = 1; freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\in.txt\", \"r\", stdin); freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\out.txt\", \"w\", stdout); scanf(\"%d\", &amp;T); //mem(sum, 0); while (T--) &#123; scanf(\"%d%d\", &amp;length, &amp;q); scanf(\"%s\", A+1); Build(1, length, 1); //for (int i = 1; i &lt; 6; i++) // printf(\"sum[%d].ch = %c sum[%d].number = %d \", i, sum[i].ch, i, sum[i].number); //printf(\"\\n\"); printf(\"Case #%d:\\n\", kcase++); solve(); &#125; return 0;&#125;","path":"2019/01/18/HDU6345/","date":"01-18","excerpt":"","tags":[{"name":"线段树","slug":"线段树","permalink":"https://github.com/JJXiangJiaoJun/tags/线段树/"},{"name":"前缀和","slug":"前缀和","permalink":"https://github.com/JJXiangJiaoJun/tags/前缀和/"}]},{"title":"2018百度之星程序设计大赛（资格赛） 调查问卷 HDU6344 解题思路","text":"题目链接 HDU 6344 调查问卷目录 本人算法竞赛小菜鸡一只，这也是我第一篇博客，希望能和网上的各路大神分享自己的思路，在交流中不断进步！ 话不多说，我来说说我对这道题目的思路： 1、题目分析 本题主要意思就是存在 n 份问卷，每份问卷存在 m 个问题，求出满足至少有 k 对问卷不同的问题子集个数。首先我们看输入的数量级，1≤m≤10 , 可以知道枚举问题子集的最多个数为 2^10=1024，状态空间数量很少，所以不难想出，大体思路如下：我们可以通过枚举子集然后判断其是否满足答案条件来进行统计即可得出答案。 2、细节思路上面分析了主要思路就是通过枚举问题的所有子集，对每次情况判断之后进行统计。 枚举思路：我采用的是刘汝佳紫书里面的增量式递归枚举，枚举 不要的问题 集合 判断思路：我们对一个问题子集，统计出一共有多少种不同的问卷，以及有每种问卷都有多少份。此时计算公式如下： 不同的问卷对数$k=\\sum (number[i]*number[j])$ 其中 i、j分别表示问卷的种数，number[i],number[j]分别表示i，j问卷的份数。例如：问卷集合为 AAA,BBB,AAA,ABC,BBB，则number[‘AAA’] = 2,number[BBB]=2,number[ABC]=1 k=number[‘AAA’]number[‘BBB’]+number[‘AAA’]number[‘ABC’]+number[‘BBB’]*number[‘ABC’] 那么现在的问题是对于一个问题集合：如何统计有多少种不同的问卷答案呢？我的思路如下：1、由于问题答案只有‘A’，‘B’两种答案，所以我们可以采用二进制编码，每份问卷的二进制编码即为该份问卷ID，例如 AAABBB，即编码为000111，编码过程中，如果该问题被删除，我们则不对该位进行编码，跳过即可。编码函数如下: 12345678//获得索引为 index份 问卷的idinline int get_id(int index)&#123; int id = 0; for (int i = 0; i &lt; m; i++) if (!is_del[i]) id = id &lt;&lt; 1 | (Q[index][i] - 'A'); return id;&#125; 当两份问卷ID相同时，即代表这两份问卷是相同的。这样我们所有的问题ID为 0000000000———————1111111111 最多为2^10=1024个状态，用一个数组int my_set[1 &lt;&lt; 12] 来保存，其中 my_set[i] 表示ID=i 的问卷个数。 我们现在已经统计完成每个ID的问卷份数,，只要对其运用上面说的公式求和，之后判断是否满足条件即可，代码如下 123456789int ans = 0;for (int i = 0; i&lt;count - 1; i++) for (int j = i + 1; j &lt; count; j++) &#123; ans += my_set[exist[i]] * my_set[exist[j]]; if (ans &gt;= k) return true; &#125;return false; 3、算法设计通过上面分析，大体思路我们已经了解，穷举+判断，继续深入想想，其实我们还可以对每种情况进行剪枝。剪枝情况如下： 1、如果当前枚举子集不符合条件，那么我们就不用继续枚举当前子集的子集了。例如当我们选择1,3,4,5,6问题时不满足情况，那么1,3,4,5,6情况的子集肯定也不满足情况，我们应当减去当前分支，直接返回。 2、对于n份问卷，最大的对数为 ,若 &lt;k则不用判断直接输出0即可。 (我的想法其实还有一种最优性剪枝，但是加上之后不知道为什么答案是错的，求各路大神指点一下。) 思路如下：假设当前问题集合有 cur 个问题，那么当前问题集合能产生的不同问题种数最多为 2^cur 个，记2^cur = num_of_set。分为两种情况： 1、num_of_set &gt; n 此时这种情况和上面剪枝的2情况一样，最优的是n份问卷每份问卷都不一样。 2、num_of_set &lt; n 此时这种情况,按照我们上面的公式，应该是在每个 set 里面份数一样时 k 能取的最大值，例如假设 n可以整除num_of_set，如number_per_set = n/num_of_set = 2 ,则当每种问题中问卷份数都是一样时等于2时，k将取最大值，计算公式为: k = 22 即总对数=选择两个不同集合第一个集合份数第二个集合份数 ,当集合份数相等时k取最大值。 若不能整除 我就让 number_per_set = n / num_of_set + 1 计算 k ，若 k小于要求值，则剪枝. 具体剪枝代码如下: 12345678910111213141516bool need_cut(int cur)&#123; //最优性剪枝,当前最大能产生的 不同问卷的对数为 int max_select = m - cur - 2; int max_set = pow(2, m - cur - 1); max_select = pow(4, max_select); //printf(\"max_select = %d\\n\", max_select); // if (n&gt;max_set) &#123; if ((max_select*(n / max_set+1)*(n / max_set+1)) &lt; k) return true; &#125; return false;&#125; 4、程序代码其实这题不用剪枝也能AC，全部的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;set&gt;#include &lt;algorithm&gt;#include &lt;cmath&gt;using namespace std;const int maxn = 1000 + 10;const int maxquestion = 10 + 5;int T;int n, m, k;char Q[maxn][maxquestion];int P[maxquestion];int is_del[maxquestion];int select[maxquestion];//保存每种情况有多少可能int my_set[1 &lt;&lt; 12];int id[maxn];int exist[maxn];int Cn2;//获得索引为 index份 问卷的idinline int get_id(int index)&#123; int id = 0; for (int i = 0; i &lt; m; i++) if (!is_del[i]) id = id &lt;&lt; 1 | (Q[index][i] - 'A'); return id;&#125;//判断函数，cur表示当前有 cur-1 个 问题已经被丢弃，将要枚举丢弃 cur 个问题bool need_cut(int cur)&#123; //最优性剪枝,当前最大能产生的 不同问卷的对数为 int max_select = m - cur - 2; int max_set = pow(2, m - cur - 1); max_select = pow(4, max_select); //printf(\"max_select = %d\\n\", max_select); // if (n&gt;max_set) &#123; if ((max_select*(n / max_set+1)*(n / max_set+1)) &lt; k) return true; &#125; return false;&#125;bool judge(int cur)&#123; memset(is_del, 0, sizeof(is_del)); memset(my_set, 0, sizeof(my_set)); int count = 0; //进行判断 for (int i = 0; i &lt; cur; i++) is_del[P[i]] = 1; //获得每个问题的id,并统计集合 for (int i = 0; i &lt; n; i++) &#123; id[i] = get_id(i); //判断是否重复 if (my_set[id[i]] == 0) &#123; exist[count] = id[i]; count++; &#125; //统计次数+1 my_set[id[i]] += 1; &#125; int ans = 0; for (int i = 0; i&lt;count - 1; i++) for (int j = i + 1; j &lt; count; j++) &#123; ans += my_set[exist[i]] * my_set[exist[j]]; if (ans &gt;= k) return true; &#125; return false;&#125;int problem_set = 0;//枚举选的问题集合，当前枚举的是第 cur 个 元素void generate_subset(int m,int cur,int mini_index)&#123; //如果需要剪枝则不继续枚举 //if (need_cut(cur)) return; //printf(\"cur = %d need_cut return false\\n\",cur); //否则进行判断 if (!judge(cur)) return; problem_set++; //if (need_cut(cur)) return; //枚举问题集合 for (int i = mini_index; i &lt; m; i++) &#123; P[cur] = i; //if (need_cut(cur)) return; //枚举下一个不选择的子集 generate_subset(m, cur + 1, i + 1); &#125; //if (need_cut(cur)) return;&#125;void solve()&#123; if (Cn2 &lt; k) &#123; printf(\"0\\n\"); return; &#125; problem_set = 0; generate_subset(m, 0, 0); printf(\"%d\\n\", problem_set);&#125;int main()&#123; freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\in.txt\", \"r\", stdin); freopen(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\test\\\\out.txt\", \"w\", stdout); scanf(\"%d\", &amp;T); int kcase = 1; while (T--) &#123; scanf(\"%d%d%d\", &amp;n, &amp;m, &amp;k); for (int i = 0; i &lt; n; i++) scanf(\"%s\", Q[i]); Cn2 = n*(n - 1) / 2; printf(\"Case #%d: \", kcase++); solve(); &#125; return 0;&#125;","path":"2019/01/18/HDU6344/","date":"01-18","excerpt":"","tags":[{"name":"暴力枚举","slug":"暴力枚举","permalink":"https://github.com/JJXiangJiaoJun/tags/暴力枚举/"},{"name":"剪枝","slug":"剪枝","permalink":"https://github.com/JJXiangJiaoJun/tags/剪枝/"}]},{"title":"深度学习论文汇总","text":"&emsp;&emsp;“读万卷书，行万里路”，深度学习领域每时每刻都在萌生新的灵感和想法。要成为这方面的大牛，我想理论知识、代码功底都得多多锻炼。我们不仅仅要对某一个方向深入了解，更要对CV这个领域有一个全面的认识。所以，读paper肯定是不能少的啦，从ImageNet比赛，到目标检测、图像分割，都有许多许多优秀的论文。这篇博客整理出一些优秀深度学习论文，也是对自己学习过程的一些记录吧，不断地学习state-of-the-art论文中的最新思想，这样才能跟得上时代的步伐吧~ 深度学习大爆发：ImageNet 挑战赛 物体检测 深度学习一些tricks以及CNN网络结构的改善 人脸检测 图像分割 GAN生成对抗网络 强化学习 深度学习大爆发：ImageNet 挑战赛&emsp;&emsp;ImageNet 挑战赛属于深度学习最基础的任务：分类。从最早最早的LeNet，到后来的GoogleNet，再到现在的Shufflenet，涌现了一大批优秀的卷积神经网络框架。这些框架也被广泛用于目标检测等更复杂的深度学习任务中作为backbone，用来提取图像的特征。各种state-of-the-art的CNN框架，也是我们首要学习的知识。 (LeNet) Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, “Gradient-based learning applied to document recognition,” in Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998.[PDF] ，CNN的开山之作，也是手写体识别经典论文 (AlexNet) Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems. 2012 [PDF]ILSVRC-2012冠军，CNN历史上的转折，也是深度学习第一次在图像识别的任务上超过了SVM等传统的机器学习方法 (VGG) Simonyan, Karen, and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).[PDF] 使用了大量的重复卷积层，对后面的网络产生了重要影响 (GoogLeNet) Szegedy, Christian, et al. Going deeper with convolutions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [PDF] 提出Inception模块，第一次在CNN中使用并行结构，后来的ResNet等都借鉴了该思想，CNN不再是一条路走到底的网络结构了 (InceptionV2、InceptionV3) Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the Inception Architecture for Computer Vision[J]. Computer Science, 2015:2818-2826.[PDF]由于BN（Batch Normalization）等提出，改进了原始GoogLeNet中的Inception模块 (ResNet) He, Kaiming, et al. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385 (2015).[PDF] 提出残差结构，解决了深度学习网络层数太深梯度消失等问题，ResNet当时的层数达到了101层。 (Xception) Chollet F. Xception: Deep Learning with Depthwise Separable Convolutions[J]. arXiv preprint arXiv:1610.02357, 2016.[PDF] (DenseNet) Huang G, Liu Z, Weinberger K Q, et al. Densely Connected Convolutional Networks[J]. 2016. [PDF] 将shortcut思想发挥到极致 (SeNet) Squeeze-and-Excitation Networks. [PDF] 主打融合通道间的信息(channel-wise)，并且只增加微量计算 (Shufflenet) Zhang X, Zhou X, Lin M, et al. Shufflenet: An extremely efficient convolutional neural network for mobile devices[J]. [PDF] 使用shuffle操作来代替1x1卷积，实现通道信息融合，大大减小了参数量，主要面向一些计算能力不足的移动设备。 (capsules) Sabour S, Frosst N, Hinton G E. Dynamic routing between capsules[C][PDF]— 物体检测&emsp;&emsp;深度学习另外一个重要的任务就是物体检测，在1990年以前，典型的物体检测方法是基于 geometric representations，之后物体检测的方法像统计分类的方向发展(神经网络、SVM、Adaboost等）。&emsp;&emsp;2012年当深度神经网络(DCNN)在图像分类上取得了突破性进展时，这个巨大的成功也被用到了物体检测上。Girshick提出了里程碑式的物体检测模型Region based CNN(RCNN)，在此之后物体检测领域飞速发展、并且提出了许多基于深度学习的方法，如YOLO、SSD等…… (R-CNN) Girshick, Ross, et al. Rich feature hierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.[PDF] 里程碑式的物体检测框架，RCNN系列的开山鼻祖，后续深度学习的物体检测都借鉴了思想，不得不读的paper (SPPNet) He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//European Conference on Computer Vision. Springer International Publishing, 2014: 346-361.[PDF] 主要改进了R-CNN中计算过慢重复提取特征的问题 (Fast R-CNN) Girshick R. Fast r-cnn[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1440-1448.[PDF] RCNN系列的第二版，提出RoI Pooling，同时改进了 R-CNN 和 SPPNet，同时提高了速度和精度 (Faster R-CNN) Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99.[PDF] R-CNN系列巅峰，提出了anchor、RPN等方法，广泛被后续网络采用。 (YOLO) Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 779-788.[PDF] One-Stage目标检测框架代表之一，速度非常快，不过精度不如R-CNN系列 (SSD) Liu W, Anguelov D, Erhan D, et al. SSD: Single shot multibox detector[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 21-37.[PDF] One-Stage目标检测框架代表之二，提高速度的同时又不降低精度 (R-FCN) Li Y, He K, Sun J. R-fcn: Object detection via region-based fully convolutional networks[C]//Advances in Neural Information Processing Systems. 2016: 379-387.[PDF] (DSSD) Fu, C., Liu, W., Ranga, A., Tyagi, A., &amp; Berg, A.C. (2017). DSSD : Deconvolutional Single Shot Detector. CoRR, abs/1701.06659.[PDF] 和FPN的思想有类似，采用deconvolution，进行了特征融合，提高了SSD在小物体，重叠物体上的检测精度 (FPN) T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan and S. Belongie, “Feature Pyramid Networks for Object Detection,” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, 2017 [PDF] 提出特征图金字塔，让卷积神经网络中深层中提取的语义信息融合到每一层的特征图中（特别是底层的高分辨率特征图也能获得高层的语义信息），提高特征图多尺度的表达，提高了一些小目标的识别精度，在图像分割和物体监测中都可用到。 (RetinaNet) Lin, T., Goyal, P., Girshick, R.B., He, K., &amp; Dollár, P. (2017). Focal Loss for Dense Object Detection. 2017 IEEE International Conference on Computer Vision (ICCV), 2999-3007.[PDF] 提出了FocalLoss 解决物体检测中负类样本过多，类别不平衡的问题 (TDM) Shrivastava, Abhinav, Rahul Sukthankar, Jitendra Malik and Abhinav Gupta. “Beyond Skip Connections: Top-Down Modulation for Object Detection.” CoRR abs/1612.06851 (2016): n. pag.[PDF] 和FPN思想类似，不过文中提出的方法是一层一层的添加top-down模块 (YOLO-v2) Redmon, J., &amp; Farhadi, A. (2017). YOLO9000: Better, Faster, Stronger. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 6517-6525.[PDF] YOLO的改进版本 (SIN) Liu, Y., Wang, R., Shan, S., &amp; Chen, X. (2018). Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships. CVPR.[PDF] 将RNN用于CV中，很新颖的网络结构 (STDN) Scale-Transferrable Object Detection Peng Zhou, Bingbing Ni, Cong Geng, Jianguo Hu, Yi Xu; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 528-537 [PDF] 主要是改进DSSD、FPN等结构中由于特征融合而引入额外参数，导致速度变慢问题。提出用DenseNet作为Backbone 从而在forward 时候进行特征融合，并提出了不带参数的scale-transform module ，保证精度的同时提高速度 (RefineDet) Shrivastava, Abhinav et al. “Training Region-Based Object Detectors with Online Hard Example Mining.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 761-769.[PDF] 结合了one-stage 和 two-stage的优点 深度学习一些tricks以及CNN网络结构的改善 (BatchNorm) Ioffe, Sergey and Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” ICML (2015).[PDF]训练过程中的大杀器，可以加速模型收敛，并且训练过程数值更加稳定 Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li:Bag of Tricks for Image Classification with Convolutional Neural Networks. CoRR abs/1812.01187 (2018) [PDF] 系统的介绍了许多训练CNN的trick 人脸检测 (SSH) Najibi, Mahyar et al. “SSH: Single Stage Headless Face Detector.” 2017 IEEE International Conference on Computer Vision (ICCV) (2017): 4885-4894. [PDF] ($S^3$FD) Zhang, S., Zhu, X., Lei, Z., Shi, H., Wang, X., &amp; Li, S.Z. (2017). S^3FD: Single Shot Scale-Invariant Face Detector. 2017 IEEE International Conference on Computer Vision (ICCV), 192-201. [PDF] 使用了锚框匹配策略和max-out增大了小尺寸人脸的召回率和假正例 图像分割GAN生成对抗网络强化学习参考博客：https://blog.csdn.net/qq_21190081/article/details/69564634","path":"2019/01/18/深度学习论文汇总/","date":"01-18","excerpt":"&emsp;&emsp;“读万卷书，行万里路”，深度学习领域每时每刻都在萌生新的灵感和想法。要成为这方面的大牛，我想理论知识、代码功底都得多多锻炼。我们不仅仅要对某一个方向深入了解，更要对CV这个领域有一个全面的认识。所以，读paper肯定是不能少的啦，从ImageNet比赛，到目标检测、图像分割，都有许多许多优秀的论文。这篇博客整理出一些优秀深度学习论文，也是对自己学习过程的一些记录吧，不断地学习state-of-the-art论文中的最新思想，这样才能跟得上时代的步伐吧~","tags":[{"name":"论文","slug":"论文","permalink":"https://github.com/JJXiangJiaoJun/tags/论文/"},{"name":"深度学习","slug":"深度学习","permalink":"https://github.com/JJXiangJiaoJun/tags/深度学习/"}]}]}